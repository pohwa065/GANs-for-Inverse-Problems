import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

def save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz'):
    """
    Save the x_grid, estimated PDF, and CDF to a file.
    
    Parameters:
        x_grid (np.ndarray): The grid of x values.
        pdf_values (np.ndarray): Estimated PDF values corresponding to x_grid.
        cdf_values (np.ndarray): Estimated CDF values corresponding to x_grid.
        filename (str): The filename to save the data.
    """
    np.savez(filename, x_grid=x_grid, pdf=pdf_values, cdf=cdf_values)
    print(f"PDF and CDF saved to {filename}")

def load_pdf_cdf(filename='pdf_cdf_data.npz'):
    """
    Load the x_grid, PDF, and CDF from a file.
    
    Parameters:
        filename (str): The filename from which to load the data.
    
    Returns:
        tuple: (x_grid, pdf_values, cdf_values)
    """
    data = np.load(filename)
    return data['x_grid'], data['pdf'], data['cdf']

def sample_from_saved_cdf(n_samples, filename='pdf_cdf_data.npz'):
    """
    Sample new values from the saved PDF/CDF using inverse transform sampling.
    
    Parameters:
        n_samples (int): Number of samples to generate.
        filename (str): The filename from which the PDF/CDF data is loaded.
    
    Returns:
        np.ndarray: Array of samples drawn according to the saved distribution.
    """
    x_grid, _, cdf = load_pdf_cdf(filename)
    # Generate uniform samples between 0 and 1
    uniform_samples = np.random.uniform(0, 1, n_samples)
    # Inverse transform: interpolate the x value corresponding to each CDF value.
    samples = np.interp(uniform_samples, cdf, x_grid)
    return samples

# -----------------------------------------------
# Example usage:
# Assume we have already computed samples for X = A + B + C
# where A ~ Gaussian, B ~ Poisson, C ~ Gamma.
# We then estimated the PDF using KDE and computed an empirical CDF.

# For demonstration, we reuse our earlier simulation code.
# Define parameters for each distribution:
mu_A, sigma_A = 0, 1          # Gaussian A
lambda_B = 3                # Poisson B
shape_C, scale_C = 2, 2       # Gamma C

# Number of samples for simulation
N = 1000000

# Generate independent samples:
samples_A = np.random.normal(mu_A, sigma_A, N)
samples_B = np.random.poisson(lambda_B, N)
samples_C = np.random.gamma(shape_C, scale_C, N)
samples = samples_A + samples_B + samples_C

# Estimate the PDF using a Gaussian KDE:
kde = gaussian_kde(samples)
x_grid = np.linspace(np.min(samples), np.max(samples), 1000)
pdf_values = kde(x_grid)

# Compute the empirical CDF:
sorted_samples = np.sort(samples)
cdf_values = np.array([np.searchsorted(sorted_samples, x, side='right') / len(sorted_samples) 
                        for x in x_grid])

# Save the estimated PDF and CDF for future sampling:
save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz')

# Later (or in another script), sample new values from the saved PDF/CDF:
n_new_samples = 10000
samples_new = sample_from_saved_cdf(n_new_samples, filename='pdf_cdf_data.npz')

# Verify the new samples by comparing their histogram with the saved PDF:
print("New samples: mean = {:.3f}, std = {:.3f}".format(np.mean(samples_new), np.std(samples_new)))

plt.hist(samples_new, bins=50, density=True, alpha=0.6, label='Sampled Histogram')
plt.plot(x_grid, pdf_values, label='Saved PDF', color='red')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram of New Samples vs. Saved PDF')
plt.legend()
plt.show()







=======
import numpy as np
from scipy.optimize import minimize_scalar

def compute_top_extremes(mu_A, sigma_A, lambda_B, k_C, theta_C, num_samples, top_n=10):
    """
    Compute approximate top extreme values analytically for sum of Gaussian(A), Poisson(B), Gamma(C).

    Parameters:
        mu_A, sigma_A: Mean and std deviation for Gaussian distribution
        lambda_B: Lambda for Poisson distribution
        k_C, theta_C: Shape and scale for Gamma distribution
        num_samples: Total number of samples drawn (e.g., 1e12)
        top_n: Number of top extremes desired

    Returns:
        np.array: Array of approximate top extreme values
    """

    # Moment Generating Functions (MGFs)
    def mgf(t):
        if t >= 1/theta_C:
            return np.inf
        mgf_A = np.exp(mu_A * t + 0.5 * sigma_A**2 * t**2)
        mgf_B = np.exp(lambda_B * (np.exp(t) - 1))
        mgf_C = (1 - theta_C * t) ** (-k_C)
        return mgf_A * mgf_B * mgf_C

    # Chernoff bound: saddle-point to compute tail exponent alpha
    def saddle_point(t):
        return np.log(mgf(t)) - t

    res = minimize_scalar(lambda t: saddle_point(t), bounds=(0, 1/(2*theta_C)), method='bounded')

    if not res.success:
        raise RuntimeError("Failed to numerically compute tail exponent alpha")

    t_opt = res.x
    alpha = t_opt

    # EVT Gumbel approximation for exponential-like tail
    gamma_euler = 0.5772156649
    extreme_location = (np.log(num_samples) + gamma_euler + np.log(np.log(num_samples))) / alpha

    # Spacing for top_n extremes is approximately O(1/alpha)
    spacing = 1 / alpha
    extremes = extreme_location + spacing * np.log(np.arange(top_n, 0, -1))

    return extremes

# Example usage:
if __name__ == '__main__':
    extremes = compute_top_extremes(
        mu_A=0, sigma_A=1, 
        lambda_B=5, 
        k_C=2, theta_C=2, 
        num_samples=1e12,
        top_n=10
    )

    print("Top extreme values (approx.):", extremes)



====

import numpy as np
from scipy.stats import norm, poisson, gamma, linregress

def simulate_and_estimate_alpha(mu_A, sigma_A, lambda_B, k_C, theta_C, sample_size=int(1e8), tail_fraction=1e-4):
    """
    Simulate samples to estimate tail exponent alpha reliably.

    Parameters:
        mu_A, sigma_A: Parameters for Gaussian
        lambda_B: Parameter for Poisson
        k_C, theta_C: Parameters for Gamma
        sample_size: Number of samples to simulate
        tail_fraction: Fraction of top samples to use for tail fitting

    Returns:
        float: Estimated tail exponent alpha
    """
    np.random.seed(42)
    
    # Generate samples
    sample_A = np.random.normal(mu_A, sigma_A, sample_size)
    sample_B = np.random.poisson(lambda_B, sample_size)
    sample_C = np.random.gamma(k_C, theta_C, sample_size)

    samples = sample_A + sample_B + sample_C

    # Sort samples to get tail
    sorted_samples = np.sort(samples)
    tail_start_idx = int((1 - tail_fraction) * sample_size)
    tail_samples = sorted_samples[tail_start_idx:]

    # Empirical survival probabilities
    survival_probs = np.linspace(tail_fraction, 1/sample_size, len(tail_samples))

    # Linear regression on log-tail probabilities vs tail samples
    slope, intercept, _, _, _ = linregress(tail_samples, np.log(survival_probs))

    alpha_estimated = -slope

    return alpha_estimated

# Example usage:
if __name__ == '__main__':
    alpha = simulate_and_estimate_alpha(
        mu_A=0, sigma_A=1,
        lambda_B=5,
        k_C=2, theta_C=2,
        sample_size=int(1e8),
        tail_fraction=1e-4
    )

    print("Estimated tail exponent alpha:", alpha)



=========
import numpy as np

def compute_top_extreme_from_tail(tail_exponent, num_samples, tail_type='exponential', top_n=10):
    """
    Compute approximate top extremes based on estimated tail exponent.

    Parameters:
        tail_exponent: alpha for exponential tails or beta for Gaussian tails
        num_samples: Total number of samples (e.g., 1e12)
        tail_type: 'exponential' or 'gaussian'
        top_n: number of extreme values required

    Returns:
        np.array: approximate top extreme values
    """

    if tail_type == 'exponential':
        extreme_location = np.log(num_samples) / tail_exponent
        spacing = 1 / tail_exponent
        extremes = extreme_location + spacing * np.log(np.arange(top_n, 0, -1))
        
    elif tail_type == 'gaussian':
        extreme_location = np.sqrt(np.log(num_samples) / tail_exponent)
        spacing = (1 / (2 * extreme_location * tail_exponent))
        extremes = extreme_location + spacing * np.log(np.arange(top_n, 0, -1))

    else:
        raise ValueError("tail_type must be 'exponential' or 'gaussian'")

    return extremes

# Example:
alpha = 0.5  # Exponential tail estimated
beta = 0.05  # Gaussian tail estimated
num_samples = 1e12

extremes_exp = compute_top_extreme_from_tail(alpha, num_samples, tail_type='exponential', top_n=10)
extremes_gauss = compute_top_extreme_from_tail(beta, num_samples, tail_type='gaussian', top_n=10)

print("Top extremes (Exponential tail):", extremes_exp)
print("Top extremes (Gaussian tail):", extremes_gauss)

====
import numpy as np
from scipy.stats import linregress

def estimate_gaussian_tail(mu_A, sigma_A, lambda_B, k_C, theta_C, sample_size=int(1e8), tail_fraction=1e-4):
    """
    Estimate Gaussian-like tail parameter beta by numerical simulation.

    Parameters:
        mu_A, sigma_A: Parameters for Gaussian
        lambda_B: Parameter for Poisson
        k_C, theta_C: Parameters for Gamma
        sample_size: Number of samples to simulate
        tail_fraction: Fraction of top samples to use for tail fitting

    Returns:
        float: Estimated Gaussian tail exponent beta
    """
    np.random.seed(42)

    # Generate samples
    sample_A = np.random.normal(mu_A, sigma_A, sample_size)
    sample_B = np.random.poisson(lambda_B, sample_size)
    sample_C = np.random.gamma(k_C, theta_C, sample_size)

    samples = sample_A + sample_B + sample_C

    # Sort samples to extract tail
    sorted_samples = np.sort(samples)
    tail_start_idx = int((1 - tail_fraction) * sample_size)
    tail_samples = sorted_samples[tail_start_idx:]

    # Compute empirical survival probabilities
    survival_probs = np.linspace(tail_fraction, 1/sample_size, len(tail_samples))

    # Fit quadratic exponential tail: log P(X>x) ~ -beta x^2
    slope, intercept, _, _, _ = linregress(tail_samples**2, np.log(survival_probs))

    beta_estimated = -slope

    return beta_estimated

# Example usage:
if __name__ == '__main__':
    beta = estimate_gaussian_tail(
        mu_A=0, sigma_A=1,
        lambda_B=2,
        k_C=0.5, theta_C=0.1,  # Gamma tail weaker than Gaussian
        sample_size=int(1e8),
        tail_fraction=1e-4
    )

    print("Estimated Gaussian-like tail exponent beta:", beta)


==========

import numpy as np
import pandas as pd
from scipy.stats import gamma, poisson, norm

def compare_extreme_SNR(k, theta, lambda_B, mu_A, sigma_A, num_samples=int(1e7), quantile=1-1e-6):
    np.random.seed(42)
    
    # Pure Gamma
    samples_gamma = np.random.gamma(k, theta, num_samples)
    extreme_gamma = np.quantile(samples_gamma, quantile)
    std_gamma = np.std(samples_gamma)
    SNR_gamma = extreme_gamma / std_gamma
    
    # Combined (Gamma + Poisson + Gaussian)
    samples_combined = (
        np.random.gamma(k, theta, num_samples) +
        np.random.poisson(lambda_B, num_samples) +
        np.random.normal(mu_A, sigma_A, num_samples)
    )
    extreme_combined = np.quantile(samples_combined, quantile)
    std_combined = np.std(samples_combined)
    SNR_combined = extreme_combined / std_combined
    
    result = {
        'Pure Gamma': {'Extreme_Value': extreme_gamma, 'Std': std_gamma, 'SNR': SNR_gamma},
        'Combined': {'Extreme_Value': extreme_combined, 'Std': std_combined, 'SNR': SNR_combined}
    }
    
    return result

# Define a function to compare SNR for different scale factors
def compare_snr_scaled_gamma(scale_factor, k=5, theta=3, lambda_B=4, mu_A=2, sigma_A=2, num_samples=int(1e7), quantile=1-1e-6):
    theta_pure = scale_factor * theta
    result = compare_extreme_SNR(k, theta_pure, lambda_B, mu_A, sigma_A, num_samples, quantile)
    
    df_result = pd.DataFrame(result).T.reset_index().rename(columns={'index': 'Scenario'})
    df_result['Gamma_Scale_Factor'] = scale_factor
    return df_result

# Test for multiple scale factors
scale_factors = [1, 2, 3, 4, 5, 6, 7]
all_results = pd.concat([compare_snr_scaled_gamma(sf) for sf in scale_factors], ignore_index=True)

import ace_tools as tools; tools.display_dataframe_to_user(name="SNR Comparison Across Gamma Scale Factors", dataframe=all_results)


======

import numpy as np
from scipy.stats import norm, poisson, gamma, linregress
import pandas as pd

def estimate_alpha_sensitivity_with_fraction(mu_A, sigma_A, lambda_B, k_C, theta_C, sample_size=int(1e7), tail_fractions=None):
    """
    Simulate samples and estimate alpha over a range of tail fractions to assess sensitivity.

    Parameters:
        mu_A, sigma_A: Gaussian parameters
        lambda_B: Poisson parameter
        k_C, theta_C: Gamma parameters
        sample_size: Total samples to simulate
        tail_fractions: List or array of tail fractions (e.g., [1e-2, 1e-3, ...])

    Returns:
        DataFrame: Tail Fraction, Threshold u, and Estimated alpha
    """
    if tail_fractions is None:
        tail_fractions = np.logspace(-5, -2, 10)  # from 1e-5 to 1e-2

    np.random.seed(42)
    # Simulate data
    sample_A = np.random.normal(mu_A, sigma_A, sample_size)
    sample_B = np.random.poisson(lambda_B, sample_size)
    sample_C = np.random.gamma(k_C, theta_C, sample_size)
    samples = sample_A + sample_B + sample_C
    sorted_samples = np.sort(samples)

    results = []

    for tail_fraction in tail_fractions:
        tail_start_idx = int((1 - tail_fraction) * sample_size)
        tail_samples = sorted_samples[tail_start_idx:]
        survival_probs = np.linspace(tail_fraction, 1/sample_size, len(tail_samples))
        
        slope, intercept, _, _, _ = linregress(tail_samples, np.log(survival_probs))
        alpha_estimated = -slope
        threshold_u = sorted_samples[tail_start_idx]
        results.append((tail_fraction, threshold_u, alpha_estimated))

    df_alpha = pd.DataFrame(results, columns=["Tail_Fraction", "Threshold_u", "Estimated_alpha"])
    return df_alpha

# Run alpha sensitivity analysis with tail fraction included
df_alpha_sensitivity = estimate_alpha_sensitivity_with_fraction(
    mu_A=0, sigma_A=1,
    lambda_B=5,
    k_C=2, theta_C=2,
    sample_size=int(1e7)
)

import ace_tools as tools; tools.display_dataframe_to_user(name="Tail Sensitivity: Fraction, Threshold, Alpha", dataframe=df_alpha_sensitivity)


