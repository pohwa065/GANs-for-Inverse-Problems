import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

def save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz'):
    """
    Save the x_grid, estimated PDF, and CDF to a file.
    
    Parameters:
        x_grid (np.ndarray): The grid of x values.
        pdf_values (np.ndarray): Estimated PDF values corresponding to x_grid.
        cdf_values (np.ndarray): Estimated CDF values corresponding to x_grid.
        filename (str): The filename to save the data.
    """
    np.savez(filename, x_grid=x_grid, pdf=pdf_values, cdf=cdf_values)
    print(f"PDF and CDF saved to {filename}")

def load_pdf_cdf(filename='pdf_cdf_data.npz'):
    """
    Load the x_grid, PDF, and CDF from a file.
    
    Parameters:
        filename (str): The filename from which to load the data.
    
    Returns:
        tuple: (x_grid, pdf_values, cdf_values)
    """
    data = np.load(filename)
    return data['x_grid'], data['pdf'], data['cdf']

def sample_from_saved_cdf(n_samples, filename='pdf_cdf_data.npz'):
    """
    Sample new values from the saved PDF/CDF using inverse transform sampling.
    
    Parameters:
        n_samples (int): Number of samples to generate.
        filename (str): The filename from which the PDF/CDF data is loaded.
    
    Returns:
        np.ndarray: Array of samples drawn according to the saved distribution.
    """
    x_grid, _, cdf = load_pdf_cdf(filename)
    # Generate uniform samples between 0 and 1
    uniform_samples = np.random.uniform(0, 1, n_samples)
    # Inverse transform: interpolate the x value corresponding to each CDF value.
    samples = np.interp(uniform_samples, cdf, x_grid)
    return samples

# -----------------------------------------------
# Example usage:
# Assume we have already computed samples for X = A + B + C
# where A ~ Gaussian, B ~ Poisson, C ~ Gamma.
# We then estimated the PDF using KDE and computed an empirical CDF.

# For demonstration, we reuse our earlier simulation code.
# Define parameters for each distribution:
mu_A, sigma_A = 0, 1          # Gaussian A
lambda_B = 3                # Poisson B
shape_C, scale_C = 2, 2       # Gamma C

# Number of samples for simulation
N = 1000000

# Generate independent samples:
samples_A = np.random.normal(mu_A, sigma_A, N)
samples_B = np.random.poisson(lambda_B, N)
samples_C = np.random.gamma(shape_C, scale_C, N)
samples = samples_A + samples_B + samples_C

# Estimate the PDF using a Gaussian KDE:
kde = gaussian_kde(samples)
x_grid = np.linspace(np.min(samples), np.max(samples), 1000)
pdf_values = kde(x_grid)

# Compute the empirical CDF:
sorted_samples = np.sort(samples)
cdf_values = np.array([np.searchsorted(sorted_samples, x, side='right') / len(sorted_samples) 
                        for x in x_grid])

# Save the estimated PDF and CDF for future sampling:
save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz')

# Later (or in another script), sample new values from the saved PDF/CDF:
n_new_samples = 10000
samples_new = sample_from_saved_cdf(n_new_samples, filename='pdf_cdf_data.npz')

# Verify the new samples by comparing their histogram with the saved PDF:
print("New samples: mean = {:.3f}, std = {:.3f}".format(np.mean(samples_new), np.std(samples_new)))

plt.hist(samples_new, bins=50, density=True, alpha=0.6, label='Sampled Histogram')
plt.plot(x_grid, pdf_values, label='Saved PDF', color='red')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram of New Samples vs. Saved PDF')
plt.legend()
plt.show()







=======
import numpy as np
from scipy.optimize import minimize_scalar

def compute_top_extremes(mu_A, sigma_A, lambda_B, k_C, theta_C, num_samples, top_n=10):
    """
    Compute approximate top extreme values analytically for sum of Gaussian(A), Poisson(B), Gamma(C).

    Parameters:
        mu_A, sigma_A: Mean and std deviation for Gaussian distribution
        lambda_B: Lambda for Poisson distribution
        k_C, theta_C: Shape and scale for Gamma distribution
        num_samples: Total number of samples drawn (e.g., 1e12)
        top_n: Number of top extremes desired

    Returns:
        np.array: Array of approximate top extreme values
    """

    # Moment Generating Functions (MGFs)
    def mgf(t):
        if t >= 1/theta_C:
            return np.inf
        mgf_A = np.exp(mu_A * t + 0.5 * sigma_A**2 * t**2)
        mgf_B = np.exp(lambda_B * (np.exp(t) - 1))
        mgf_C = (1 - theta_C * t) ** (-k_C)
        return mgf_A * mgf_B * mgf_C

    # Chernoff bound: saddle-point to compute tail exponent alpha
    def saddle_point(t):
        return np.log(mgf(t)) - t

    res = minimize_scalar(lambda t: saddle_point(t), bounds=(0, 1/(2*theta_C)), method='bounded')

    if not res.success:
        raise RuntimeError("Failed to numerically compute tail exponent alpha")

    t_opt = res.x
    alpha = t_opt

    # EVT Gumbel approximation for exponential-like tail
    gamma_euler = 0.5772156649
    extreme_location = (np.log(num_samples) + gamma_euler + np.log(np.log(num_samples))) / alpha

    # Spacing for top_n extremes is approximately O(1/alpha)
    spacing = 1 / alpha
    extremes = extreme_location + spacing * np.log(np.arange(top_n, 0, -1))

    return extremes

# Example usage:
if __name__ == '__main__':
    extremes = compute_top_extremes(
        mu_A=0, sigma_A=1, 
        lambda_B=5, 
        k_C=2, theta_C=2, 
        num_samples=1e12,
        top_n=10
    )

    print("Top extreme values (approx.):", extremes)



====

import numpy as np
from scipy.stats import norm, poisson, gamma, linregress

def simulate_and_estimate_alpha(mu_A, sigma_A, lambda_B, k_C, theta_C, sample_size=int(1e8), tail_fraction=1e-4):
    """
    Simulate samples to estimate tail exponent alpha reliably.

    Parameters:
        mu_A, sigma_A: Parameters for Gaussian
        lambda_B: Parameter for Poisson
        k_C, theta_C: Parameters for Gamma
        sample_size: Number of samples to simulate
        tail_fraction: Fraction of top samples to use for tail fitting

    Returns:
        float: Estimated tail exponent alpha
    """
    np.random.seed(42)
    
    # Generate samples
    sample_A = np.random.normal(mu_A, sigma_A, sample_size)
    sample_B = np.random.poisson(lambda_B, sample_size)
    sample_C = np.random.gamma(k_C, theta_C, sample_size)

    samples = sample_A + sample_B + sample_C

    # Sort samples to get tail
    sorted_samples = np.sort(samples)
    tail_start_idx = int((1 - tail_fraction) * sample_size)
    tail_samples = sorted_samples[tail_start_idx:]

    # Empirical survival probabilities
    survival_probs = np.linspace(tail_fraction, 1/sample_size, len(tail_samples))

    # Linear regression on log-tail probabilities vs tail samples
    slope, intercept, _, _, _ = linregress(tail_samples, np.log(survival_probs))

    alpha_estimated = -slope

    return alpha_estimated

# Example usage:
if __name__ == '__main__':
    alpha = simulate_and_estimate_alpha(
        mu_A=0, sigma_A=1,
        lambda_B=5,
        k_C=2, theta_C=2,
        sample_size=int(1e8),
        tail_fraction=1e-4
    )

    print("Estimated tail exponent alpha:", alpha)
