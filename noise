import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

def save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz'):
    """
    Save the x_grid, estimated PDF, and CDF to a file.
    
    Parameters:
        x_grid (np.ndarray): The grid of x values.
        pdf_values (np.ndarray): Estimated PDF values corresponding to x_grid.
        cdf_values (np.ndarray): Estimated CDF values corresponding to x_grid.
        filename (str): The filename to save the data.
    """
    np.savez(filename, x_grid=x_grid, pdf=pdf_values, cdf=cdf_values)
    print(f"PDF and CDF saved to {filename}")

def load_pdf_cdf(filename='pdf_cdf_data.npz'):
    """
    Load the x_grid, PDF, and CDF from a file.
    
    Parameters:
        filename (str): The filename from which to load the data.
    
    Returns:
        tuple: (x_grid, pdf_values, cdf_values)
    """
    data = np.load(filename)
    return data['x_grid'], data['pdf'], data['cdf']

def sample_from_saved_cdf(n_samples, filename='pdf_cdf_data.npz'):
    """
    Sample new values from the saved PDF/CDF using inverse transform sampling.
    
    Parameters:
        n_samples (int): Number of samples to generate.
        filename (str): The filename from which the PDF/CDF data is loaded.
    
    Returns:
        np.ndarray: Array of samples drawn according to the saved distribution.
    """
    x_grid, _, cdf = load_pdf_cdf(filename)
    # Generate uniform samples between 0 and 1
    uniform_samples = np.random.uniform(0, 1, n_samples)
    # Inverse transform: interpolate the x value corresponding to each CDF value.
    samples = np.interp(uniform_samples, cdf, x_grid)
    return samples

# -----------------------------------------------
# Example usage:
# Assume we have already computed samples for X = A + B + C
# where A ~ Gaussian, B ~ Poisson, C ~ Gamma.
# We then estimated the PDF using KDE and computed an empirical CDF.

# For demonstration, we reuse our earlier simulation code.
# Define parameters for each distribution:
mu_A, sigma_A = 0, 1          # Gaussian A
lambda_B = 3                # Poisson B
shape_C, scale_C = 2, 2       # Gamma C

# Number of samples for simulation
N = 1000000

# Generate independent samples:
samples_A = np.random.normal(mu_A, sigma_A, N)
samples_B = np.random.poisson(lambda_B, N)
samples_C = np.random.gamma(shape_C, scale_C, N)
samples = samples_A + samples_B + samples_C

# Estimate the PDF using a Gaussian KDE:
kde = gaussian_kde(samples)
x_grid = np.linspace(np.min(samples), np.max(samples), 1000)
pdf_values = kde(x_grid)

# Compute the empirical CDF:
sorted_samples = np.sort(samples)
cdf_values = np.array([np.searchsorted(sorted_samples, x, side='right') / len(sorted_samples) 
                        for x in x_grid])

# Save the estimated PDF and CDF for future sampling:
save_pdf_cdf(x_grid, pdf_values, cdf_values, filename='pdf_cdf_data.npz')

# Later (or in another script), sample new values from the saved PDF/CDF:
n_new_samples = 10000
samples_new = sample_from_saved_cdf(n_new_samples, filename='pdf_cdf_data.npz')

# Verify the new samples by comparing their histogram with the saved PDF:
print("New samples: mean = {:.3f}, std = {:.3f}".format(np.mean(samples_new), np.std(samples_new)))

plt.hist(samples_new, bins=50, density=True, alpha=0.6, label='Sampled Histogram')
plt.plot(x_grid, pdf_values, label='Saved PDF', color='red')
plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram of New Samples vs. Saved PDF')
plt.legend()
plt.show()







=======
import numpy as np
from scipy.optimize import minimize_scalar

def compute_top_extremes(mu_A, sigma_A, lambda_B, k_C, theta_C, num_samples, top_n=10):
    """
    Compute approximate top extreme values analytically for sum of Gaussian(A), Poisson(B), Gamma(C).

    Parameters:
        mu_A, sigma_A: Mean and std deviation for Gaussian distribution
        lambda_B: Lambda for Poisson distribution
        k_C, theta_C: Shape and scale for Gamma distribution
        num_samples: Total number of samples drawn (e.g., 1e12)
        top_n: Number of top extremes desired

    Returns:
        np.array: Array of approximate top extreme values
    """

    # Moment Generating Functions (MGFs)
    def mgf(t):
        if t >= 1/theta_C:
            return np.inf
        mgf_A = np.exp(mu_A * t + 0.5 * sigma_A**2 * t**2)
        mgf_B = np.exp(lambda_B * (np.exp(t) - 1))
        mgf_C = (1 - theta_C * t) ** (-k_C)
        return mgf_A * mgf_B * mgf_C

    # Chernoff bound: saddle-point to compute tail exponent alpha
    def saddle_point(t):
        return np.log(mgf(t)) - t

    res = minimize_scalar(lambda t: saddle_point(t), bounds=(0, 1/(2*theta_C)), method='bounded')

    if not res.success:
        raise RuntimeError("Failed to numerically compute tail exponent alpha")

    t_opt = res.x
    alpha = t_opt

    # EVT Gumbel approximation for exponential-like tail
    gamma_euler = 0.5772156649
    extreme_location = (np.log(num_samples) + gamma_euler + np.log(np.log(num_samples))) / alpha

    # Spacing for top_n extremes is approximately O(1/alpha)
    spacing = 1 / alpha
    extremes = extreme_location + spacing * np.log(np.arange(top_n, 0, -1))

    return extremes

# Example usage:
if __name__ == '__main__':
    extremes = compute_top_extremes(
        mu_A=0, sigma_A=1, 
        lambda_B=5, 
        k_C=2, theta_C=2, 
        num_samples=1e12,
        top_n=10
    )

    print("Top extreme values (approx.):", extremes)



=======
import numpy as np
from scipy.integrate import quad
from scipy.optimize import root_scalar, minimize_scalar
from scipy.stats import norm

# --- Characteristic Function Based Inversion ---
def cf_sample(t, mu_A, sigma_A, lambda_B, k_C, theta_C):
    cf_A = np.exp(1j * mu_A * t - 0.5 * sigma_A**2 * t**2)
    cf_B = np.exp(lambda_B * (np.exp(1j * t) - 1))
    cf_C = (1 - 1j * theta_C * t) ** (-k_C)
    return cf_A * cf_B * cf_C

def gil_pelaez_cdf(x, mu_A, sigma_A, lambda_B, k_C, theta_C):
    integrand = lambda t: np.imag(np.exp(-1j * t * x) * cf_sample(t, mu_A, sigma_A, lambda_B, k_C, theta_C)) / t
    integral, _ = quad(integrand, 1e-10, 100, limit=500)
    return 0.5 - (1 / np.pi) * integral

def compute_extreme_quantiles_cf(mu_A, sigma_A, lambda_B, k_C, theta_C, num_samples, top_n=10):
    target_probs = [1 - (k - 0.5) / num_samples for k in range(1, top_n + 1)]
    quantiles = []

    for p in target_probs:
        result = root_scalar(
            lambda x: gil_pelaez_cdf(x, mu_A, sigma_A, lambda_B, k_C, theta_C) - p,
            bracket=[-100, 1000], method='brentq'
        )
        if not result.converged:
            raise RuntimeError("Quantile root finding did not converge")
        quantiles.append(result.root)

    return np.array(quantiles)

# --- Saddlepoint Approximation ---
def K(t, mu_A, sigma_A, lambda_B, k_C, theta_C):
    return mu_A * t + 0.5 * sigma_A**2 * t**2 + lambda_B * (np.exp(t) - 1) - k_C * np.log(1 - theta_C * t)

def K_prime(t, mu_A, sigma_A, lambda_B, k_C, theta_C):
    return mu_A + sigma_A**2 * t + lambda_B * np.exp(t) + (k_C * theta_C) / (1 - theta_C * t)

def K_double_prime(t, sigma_A, lambda_B, k_C, theta_C):
    return sigma_A**2 + lambda_B * np.exp(t) + (k_C * theta_C**2) / (1 - theta_C * t)**2

def saddlepoint_tail_prob(q, mu_A, sigma_A, lambda_B, k_C, theta_C):
    # Solve K'(t) = q for t (saddlepoint)
    result = root_scalar(
        lambda t: K_prime(t, mu_A, sigma_A, lambda_B, k_C, theta_C) - q,
        bracket=[-0.1, 1 / (theta_C + 1e-9)], method='brentq'
    )
    if not result.converged:
        raise RuntimeError("Saddlepoint equation did not converge")
    t_hat = result.root

    K_val = K(t_hat, mu_A, sigma_A, lambda_B, k_C, theta_C)
    Kpp = K_double_prime(t_hat, sigma_A, lambda_B, k_C, theta_C)

    w = np.sign(t_hat) * np.sqrt(2 * (t_hat * q - K_val))
    u = t_hat * np.sqrt(Kpp)
    tail_prob = norm.sf(w) + norm.pdf(w) * (1/w - 1/u)
    return tail_prob

def compute_extreme_quantiles_saddlepoint(mu_A, sigma_A, lambda_B, k_C, theta_C, num_samples, top_n=10):
    target_probs = [(k - 0.5) / num_samples for k in range(1, top_n + 1)]
    quantiles = []

    for p in target_probs:
        result = root_scalar(
            lambda q: saddlepoint_tail_prob(q, mu_A, sigma_A, lambda_B, k_C, theta_C) - p,
            bracket=[-100, 1000], method='brentq'
        )
        if not result.converged:
            raise RuntimeError("Quantile root finding (saddlepoint) did not converge")
        quantiles.append(result.root)

    return np.array(quantiles)

# Example usage
if __name__ == '__main__':
    params = {
        'mu_A': 0,
        'sigma_A': 1,
        'lambda_B': 5,
        'k_C': 2,
        'theta_C': 2,
        'num_samples': 1e12,
        'top_n': 10
    }

    print("Using Characteristic Function Inversion:")
    q_cf = compute_extreme_quantiles_cf(**params)
    print(q_cf)

    print("\nUsing Saddlepoint Approximation:")
    q_sp = compute_extreme_quantiles_saddlepoint(**params)
    print(q_sp)


