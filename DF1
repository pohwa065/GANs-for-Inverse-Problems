from torchsummary import summary
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
import cv2

import torch
import torch.nn as nn
from torchsummary import summary

# Define a helper function for cropping
def crop_tensor(tensor, target_tensor):
    target_size = target_tensor.size()[2:]
    tensor_size = tensor.size()[2:]
    delta_h = tensor_size[0] - target_size[0]
    delta_w = tensor_size[1] - target_size[1]
    crop_h = delta_h // 2
    crop_w = delta_w // 2
    return tensor[:, :, crop_h:crop_h + target_size[0], crop_w:crop_w + target_size[1]]

# Define the U-Net architecture for the generator with 3x3 kernel size
class UNet(nn.Module):
    def __init__(self, kernel_size=3):
        super(UNet, self).__init__()
        self.encoder1 = nn.Sequential(
            nn.Conv2d(m, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.pool1 = nn.MaxPool2d(2, 2)
        self.encoder2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.pool2 = nn.MaxPool2d(2, 2)
        self.middle = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.decoder2 = nn.Sequential(
            nn.Conv2d(256, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.decoder1 = nn.Sequential(
            nn.Conv2d(128, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        middle = self.middle(self.pool2(enc2))
        dec2 = self.decoder2(torch.cat([self.upconv2(middle), crop_tensor(enc2, self.upconv2(middle))], dim=1))
        dec1 = self.decoder1(torch.cat([self.upconv1(dec2), crop_tensor(enc1, self.upconv1(dec2))], dim=1))
        out = self.final_conv(dec1)
        return out

# Define the discriminator with 3x3 kernel size
class Discriminator(nn.Module):
    def __init__(self, kernel_size=3):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 1, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Custom correlation loss function using PyTorch operations
def correlation_loss(output, target):
    batch_size = output.size(0)
    correlation = 0
    for i in range(batch_size):
        for j in range(m):
            output_flat = output[i, 0].view(-1)
            target_flat = target[i, j].view(-1)
            output_mean = torch.mean(output_flat)
            target_mean = torch.mean(target_flat)
            output_centered = output_flat - output_mean
            target_centered = target_flat - target_mean
            correlation_numerator = torch.sum(output_centered * target_centered)
            correlation_denominator = torch.sqrt(torch.sum(output_centered ** 2) * torch.sum(target_centered ** 2))
            correlation += correlation_numerator / (correlation_denominator + 1e-8)
    correlation /= (batch_size * m)
    return 1 - correlation

# Combined loss function for the discriminator (MSE only)
def discriminator_loss(real_output, fake_output):
    mse_loss = nn.MSELoss()(real_output, torch.ones_like(real_output)) + nn.MSELoss()(fake_output, torch.zeros_like(fake_output))
    return mse_loss

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torchsummary import summary
import os

# Define an inception block
class InceptionBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(InceptionBlock, self).__init__()
        self.branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.branch3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.branch5 = nn.Conv2d(in_channels, out_channels, kernel_size=5, padding=2)
        self.branch_pool = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        branch1 = self.branch1(x)
        branch3 = self.branch3(x)
        branch5 = self.branch5(x)
        branch_pool = self.branch_pool(nn.functional.max_pool2d(x, kernel_size=3, stride=1, padding=1))
        outputs = [branch1, branch3, branch5, branch_pool]
        return torch.cat(outputs, 1)

# Define a helper function for cropping
def crop_tensor(tensor, target_tensor):
    target_size = target_tensor.size()[2:]
    tensor_size = tensor.size()[2:]
    delta_h = tensor_size[0] - target_size[0]
    delta_w = tensor_size[1] - target_size[1]
    crop_h = delta_h // 2
    crop_w = delta_w // 2
    return tensor[:, :, crop_h:crop_h + target_size[0], crop_w:crop_w + target_size[1]]

# Define the U-Net architecture for the generator with inception blocks and additional skip connections
class UNet(nn.Module):
    def __init__(self, kernel_size=3):
        super(UNet, self).__init__()
        self.encoder1 = nn.Sequential(
            nn.Conv2d(m, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.pool1 = nn.MaxPool2d(2, 2)
        self.encoder2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.pool2 = nn.MaxPool2d(2, 2)
        self.middle = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.decoder2 = nn.Sequential(
            nn.Conv2d(256, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.decoder1 = nn.Sequential(
            nn.Conv2d(128, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=kernel_size//2),
            nn.ReLU()
        )
        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)

        # Additional inception blocks
        self.inception1 = InceptionBlock(64, 16)
        self.inception2 = InceptionBlock(128, 32)
        self.inception3 = InceptionBlock(256, 64)

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc1_inception = self.inception1(enc1)
        enc2 = self.encoder2(self.pool1(enc1_inception))
        enc2_inception = self.inception2(enc2)
        middle = self.middle(self.pool2(enc2_inception))
        middle_inception = self.inception3(middle)
        
        # Skip connections
        dec2 = self.decoder2(torch.cat([self.upconv2(middle_inception), crop_tensor(enc2_inception, self.upconv2(middle_inception))], dim=1))
        dec1 = self.decoder1(torch.cat([self.upconv1(dec2), crop_tensor(enc1_inception, self.upconv1(dec2))], dim=1))
        
        out = self.final_conv(dec1)
        return out

# Define the discriminator with configurable kernel size
class Discriminator(nn.Module):
    def __init__(self, kernel_size=3):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 1, kernel_size=kernel_size, stride=2, padding=kernel_size//2),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Custom correlation loss function using PyTorch operations
def correlation_loss(output, target):
    batch_size = output.size(0)
    correlation = 0
    for i in range(batch_size):
        for j in range(m):
            output_flat = output[i, 0].view(-1)
            target_flat = target[i, j].view(-1)
            output_mean = torch.mean(output_flat)
            target_mean = torch.mean(target_flat)
            output_centered = output_flat - output_mean
            target_centered = target_flat - target_mean
            correlation_numerator = torch.sum(output_centered * target_centered)
            correlation_denominator = torch.sqrt(torch.sum(output_centered ** 2) * torch.sum(target_centered ** 2))
            correlation += correlation_numerator / (correlation_denominator + 1e-8)
    correlation /= (batch_size * m)
    return 1 - correlation

# Gradient penalty function
def gradient_penalty(discriminator, real_images, fake_images):
    batch_size = real_images.size(0)
    epsilon = torch.rand(batch_size, 1, 1, 1, device=real_images.device)
    interpolated_images = epsilon * real_images + (1 - epsilon) * fake_images
    interpolated_images.requires_grad_(True)
    
    interpolated_output = discriminator(interpolated_images)
    gradients = torch.autograd.grad(
        outputs=interpolated_output,
        inputs=interpolated_images,
        grad_outputs=torch.ones_like(interpolated_output),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    
    gradients = gradients.view(batch_size, -1)
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()
    return gradient_penalty

# Combined loss function for the discriminator with gradient penalty
def discriminator_loss_with_gp(discriminator, real_images, fake_images, real_output, fake_output, lambda_gp=10):
    mse_loss = nn.MSELoss()(real_output, torch.ones_like(real_output)) + nn.MSELoss()(fake_output, torch.zeros_like(fake_output))
    gp = gradient_penalty(discriminator, real_images, fake_images)
    return mse_loss + lambda_gp * gp

def ___compute_focus_measure(batch_x):
    # Reshape batch_x to (batch_size * m, 1, H, W)
    batch_x_reshaped = batch_x.view(-1, 1, batch_x.size(2), batch_x.size(3))
    # Compute Laplacian
    laplacian = cv2.Laplacian(batch_x_reshaped, cv2.CV_32F, ksize=3)
    laplacian = torch.abs(laplacian)
    # Reshape back to (batch_size, m, H, W)
    laplacian = laplacian.view(batch_x.size(0), batch_x.size(1), batch_x.size(2), batch_x.size(3))
    # Compute mean focus measure across channels
    focus_measure = laplacian.mean(dim=1, keepdim=True)  # Shape: (batch_size, 1, H, W)
    # Normalize the focus measure to [0, 1]
    focus_measure = (focus_measure - focus_measure.min()) / (focus_measure.max() - focus_measure.min() + 1e-8)
    return focus_measure


def compute_focus_measure(batch_x, kernel_size=3):       # intensity based on laplacian of each z, works better 
    # Ensure the input is a PyTorch tensor
    if not isinstance(batch_x, torch.Tensor):
        raise TypeError("Input must be a PyTorch tensor")
    
    # Get the shape of the input tensor
    batch_size, m, height, width = batch_x.shape
    
    # Initialize an empty tensor to store the focus measures
    focus_measures = torch.zeros_like(batch_x)
    
    # Iterate over the batch and channels
    for b in range(batch_size):
        for c in range(m):
            # Convert the PyTorch tensor to a NumPy array
            image = batch_x[b, c].cpu().numpy()
            
            # Ensure the image is in a compatible format (e.g., 32-bit float)
            if image.dtype != np.float32:
                image = image.astype(np.float32)
            
            # Compute the focus measure using the Variance of Laplacian with the specified kernel size
            focus_measure = cv2.Laplacian(image, cv2.CV_32F, ksize=kernel_size)
            focus_measure = np.abs(focus_measure)
            
            # Convert the focus measure back to a PyTorch tensor and store it
            focus_measures[b, c] = torch.tensor(focus_measure, dtype=torch.float32)
    
    return focus_measures



def __compute_focus_measure(tensor):                     # intensity based on 2nd derivative across channels, works not as good
    # Ensure the input tensor has the correct shape
    assert tensor.ndim == 4, "Input tensor must have 4 dimensions (m, c, 96, 96)"
    
    # Define the 2x2 sum filter
    sum_filter = torch.ones((1, 1, 2, 2), device=tensor.device)
    
    # Apply the 2x2 sum filter to the input tensor
    convolved_tensor = F.conv2d(tensor.view(-1, 1, tensor.size(2), tensor.size(3)), sum_filter, stride=1, padding=0)
    convolved_tensor = convolved_tensor.view(tensor.size(0), tensor.size(1), convolved_tensor.size(2), convolved_tensor.size(3))
    
    # Ensure the convolved tensor has the correct shape
    assert convolved_tensor.size(2) == 95 and convolved_tensor.size(3) == 95, "Convolved tensor has incorrect shape"
    
    # Pad the convolved tensor to get the desired output shape
    convolved_tensor = F.pad(convolved_tensor, (1, 0, 1, 0))
    

    convolved_tensor = tensor # no need to convolve
    # Get the shape of the convolved tensor
    m, c, h, w = convolved_tensor.shape
    
    # Initialize an empty tensor to store the focus measures
    focus_measure = torch.zeros((m, 1, h, w), device=tensor.device)
    
    # Compute the second derivative across the channel direction
    for i in range(1, c-1):
        second_derivative = convolved_tensor[:, i-1, :, :] - 2 * convolved_tensor[:, i, :, :] + convolved_tensor[:, i+1, :, :]
        focus_measure[:, 0, :, :] += second_derivative
    
    focus_measure = -focus_measure  # Negate the focus measure
    # Normalize the focus measure to [0, 1]
    focus_measure = (focus_measure - focus_measure.min()) / (focus_measure.max() - focus_measure.min() + 1e-8)
    
    return focus_measure


def __focus_aware_loss(x_batch, output, target):
    # make sure the focus measure of the output and target are the same
    # x_batch: (batch_size, m, H, W), output: (batch_size, 1, H, W), target: (batch_size, 1, H, W)
    # replace the middle channel of x_batch with output and target. Create two new tensors
    x_batch_output = x_batch.clone()
    x_batch_target = x_batch.clone()
    x_batch_output[:,1,:,:] = output[:,0,:,:]
    x_batch_target[:,1,:,:] = target[:,0,:,:]
    # compute the focus measure of the new tensors
    focus_measure_output = compute_focus_measure(x_batch_output)
    focus_measure_target = compute_focus_measure(x_batch_target)
    # compute the mse loss
    mse_loss_output = F.mse_loss(output, target)
    # weighted mse loss by absolute difference of focus measures
    focus_aware_loss = mse_loss_output * torch.abs(focus_measure_output - focus_measure_target)
    return focus_aware_loss.mean()


def focus_aware_loss(output, target, focus_measure):
    
    # Compute per-pixel MSE loss
    mse_loss = F.mse_loss(output, target, reduction='none')  # Shape: (batch_size, 1, H, W)
    print(mse_loss.shape)
    print(focus_measure.shape)
    # Apply the focus measure as weights
    weighted_loss = mse_loss * focus_measure  # Element-wise multiplication
    # Compute the mean loss
    return weighted_loss.mean()


def __focus_aware_loss(output, target, focus_measure):
    # Compute per-pixel cross-entropy loss
    bce_loss = F.binary_cross_entropy_with_logits(output, target, reduction='none')
    # expand ce_loss from (batch_size, H, W) to (batch_size, 1, H, W)
    bce_loss = bce_loss.unsqueeze(1)
    #print(output)
    #print(target)
    print(bce_loss.mean())
    #mse_loss = F.mse_loss(output, target, reduction='none')  # Shape: (batch_size, 1, H, W)
    # Apply the focus measure as weights
    weighted_loss = bce_loss * focus_measure  # Element-wise multiplication
    print(focus_measure.mean())
    # Compute the mean loss
    return weighted_loss.mean()

### load data

###########################################################################

# load the data
os.chdir(r'/home/pwang/Documents/MBC/m3s_MSE0010_pol2_model3')
loaded_training_x_tensor = torch.load('pol2_training_x_tensor3.pt')
loaded_training_y_tensor = torch.load('pol2_training_y_tensor3.pt')
print(loaded_training_x_tensor.shape)
print(loaded_training_y_tensor.shape)

########### scale to X_tensor by the factor from corresponding Y_tensor
max_values = loaded_training_y_tensor.view(loaded_training_y_tensor.shape[0], -1).max(dim=1)[0]
max_values = max_values.view(-1, 1, 1, 1)
scaled_training_y_tensor = loaded_training_y_tensor / max_values
###########

########### scale to X_tensor by its 85% tile #####################
infocus = loaded_training_x_tensor[:, 1:2, :, :]
percentiles = torch.tensor(np.percentile(infocus.numpy(), 99.5, axis=(2, 3), keepdims=True)).float()

percentiles = percentiles.view(-1, 1, 1, 1)
scaled_training_x_tensor = loaded_training_x_tensor/percentiles

###################################################################

X_train = scaled_training_x_tensor  
y_train = scaled_training_y_tensor

# Initialize the models with 3x3 kernel size
m = 3  # Number of channels (configurable)
H, W = 96, 96  # Height and width of the images
kernel_size = 3  # Kernel size for capturing 2-pixel features
generator = UNet(kernel_size=kernel_size)
discriminator = Discriminator(kernel_size=kernel_size)

# Hyperparameters
learning_rate = 0.0002
num_epochs = 1000
lambda_adv = 0  # Weighting factor for adversarial loss
lambda_corr = 0  # Weighting factor for correlation loss
lambda_focus = 1  # Weighting factor for focus-aware loss
lambda_mse = 0 # weighting factor for pixel-wise MSE loss
lambda_gp = 10  # Weighting factor for gradient penalty

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))

###########################################################################


### training

import torch
from torch.utils.data import DataLoader, TensorDataset

################################
# Training loop #
batch_size = 35
###############################
g_loss_list =  []
d_loss_list = []

# use dataloaders
print('training sample: '+str(X_train.shape), 'training target: ' + str(y_train.shape))
dataset = TensorDataset(X_train, y_train)
#train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Training loop with dataloader
for epoch in range(880,num_epochs):
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    # limit to 30 batches per epoch

    #permutation = torch.randperm(X_train.size(0))
    #print('permutation: ' + str(permutation))
    #print(len(permutation))
    for i, data in enumerate(train_loader):
        if i > 85:
            break

        print(f"Batch {i}: {data[0].shape}, {data[1].shape}")
        
        X_train, y_train = data
        #print(X_train.shape, y_train.shape)
        #print('batch size: ' + str(batch_size))

        #indices = permutation[i:i + batch_size]
        #batch_x, batch_y = X_train[indices], y_train[indices]
        batch_x, batch_y = X_train, y_train
        print(batch_x.shape, batch_y.shape)
        
        # Move data to the appropriate device (if using GPU)
        #batch_x = batch_x.to(device)
        #batch_y = batch_y.to(device)

        # Train discriminator
        discriminator.train()
        d_optimizer.zero_grad()
        real_output = discriminator(batch_y)
        fake_images = generator(batch_x)
        fake_output = discriminator(fake_images.detach())
        d_loss = discriminator_loss(real_output, fake_output)
        #d_loss = discriminator_loss_with_gp(discriminator, batch_y, fake_images, real_output, fake_output, lambda_gp)
        d_loss.backward()
        d_optimizer.step()

        # Train generator
        generator.train()
        g_optimizer.zero_grad()
        fake_images = generator(batch_x)
        fake_output = discriminator(fake_images)

        ################
        # BCE loss vs. MSE loss
        #adv_loss = nn.BCELoss()(fake_output, torch.ones_like(fake_output))
        adv_loss = nn.MSELoss()(fake_output, torch.ones_like(fake_output))
        ################
        # Pixelwise MSE lost 
        mse_loss_fn = nn.MSELoss()
        pixelwise_mse_loss = mse_loss_fn(fake_images, batch_y)

        ################
        # make sure that the generated images are similar to the input images
        
        corr_loss = correlation_loss(fake_images, batch_x)
        ################
        # make sure that the mse loss is weighted more at the defect regions than the non-defect regions
        # weighting is given by the focus measure.

        focus_measure = compute_focus_measure(batch_x)
        focus_loss = focus_aware_loss(fake_images, batch_y, focus_measure)
        ################
        g_loss = lambda_adv * adv_loss + lambda_corr * corr_loss + lambda_focus * focus_loss + lambda_mse* pixelwise_mse_loss
        g_loss.backward()
        g_optimizer.step()

    print(f'Epoch [{epoch + 1}/{num_epochs}], D Loss: {d_loss.item():.6f}, G Loss: {g_loss.item():.9f}')
    # save the g loss and d loss for each epoch

    g_loss_list.append(g_loss.item())
    d_loss_list.append(d_loss.item())

    # Save the model checkpoints
    if (epoch + 1) % 10 == 0:
        torch.save(generator.state_dict(), f'generator_{epoch + 1}.pt')
        torch.save(discriminator.state_dict(), f'discriminator_{epoch + 1}.pt')
