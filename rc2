import torch
import torch.nn as nn
import torch.nn.functional as F
import math

#############################################
# Helper: Time Embedding
#############################################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generates sinusoidal embeddings for time steps.
    
    timesteps: Tensor of shape (n,) containing time step integers.
    embedding_dim: Dimension of the embedding vector.
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

#############################################
# Denoising Network (Rectified Flow, v-prediction)
#############################################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention block.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class ResidualBlockWithAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        Residual block that injects time embedding and applies channel attention.
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb):
        h = self.activation(self.conv1(x))
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        h = self.channel_attention(h)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

class DenoisingNet(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128, reduction=16):
        """
        Rectified flow network for velocity (v) prediction.
        It receives a noisy image and time step, then outputs the predicted velocity.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.downsample = nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1)
        self.resblock2 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.resblock3 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.upsample = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1)
        self.resblock4 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)
    
    def forward(self, x, t):
        """
        x: Noisy image of shape (n, in_channels, H, W)
        t: Time step tensor of shape (n,)
        """
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        h = self.conv_in(x)
        h = self.resblock1(h, t_emb)
        h = self.downsample(h)
        h = self.resblock2(h, t_emb)
        h = self.resblock3(h, t_emb)
        h = self.upsample(h)
        h = self.resblock4(h, t_emb)
        out = self.conv_out(h)
        return out  # Predicted velocity v

#############################################
# Prior Consistency Module
#############################################
class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        """
        A small network that extracts an embedding from an image.
        We use it to compare the denoised image and the prior (after downsampling).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    
    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = self.pool(h)
        h = h.view(h.size(0), -1)
        return h

#############################################
# Training Pipeline: Co-optimizing Two Models
#############################################
def train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=100,
                     lambda_prior=0.1, device="cuda"):
    """
    Co-optimizes:
      - The denoising network (using rectified flow loss).
      - The prior consistency module (enforcing that the denoised image is similar in embedding space to the prior).
    
    dataloader: yields (x0, high_res_prior) where
      • x0: Clean target image (n, 1, H, W).
      • high_res_prior: High-res conditioning image (n, 3, H_hr, W_hr).
    """
    optimizer_denoise = torch.optim.Adam(denoise_model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    
    # Define the diffusion schedule.
    betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
    alphas = 1.0 - betas
    alpha_bars = torch.cumprod(alphas, dim=0)
    
    denoise_model.train()
    prior_model.train()
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            # x0: clean low-res image (n, 1, H, W)
            # high_res_prior: high-res image (n, 3, H_hr, W_hr)
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size = x0.size(0)
            optimizer_denoise.zero_grad()
            optimizer_prior.zero_grad()
            
            # Sample a random time step t.
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            
            # Sample Gaussian noise.
            noise = torch.randn_like(x0)
            # Forward process: x_t = sqrt(alpha_bar_t)*x0 + sqrt(1-alpha_bar_t)*noise
            x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
            # Ground-truth velocity.
            v_true = torch.sqrt(alpha_bar_t) * noise - torch.sqrt(1 - alpha_bar_t) * x0
            # Denoising network prediction.
            v_pred = denoise_model(x_t, t)
            v_loss = F.mse_loss(v_pred, v_true)
            # Invert to obtain predicted clean image.
            x0_pred = torch.sqrt(alpha_bar_t) * x_t - torch.sqrt(1 - alpha_bar_t) * v_pred
            
            # Process the high-res prior: downsample to x0 resolution and convert to grayscale.
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            if prior_down.size(1) == 3 and x0.size(1) == 1:
                prior_down = prior_down.mean(dim=1, keepdim=True)
            
            # Compute embeddings.
            emb_pred = prior_model(x0_pred)
            emb_prior = prior_model(prior_down)
            consistency_loss = F.mse_loss(emb_pred, emb_prior)
            
            loss = v_loss + lambda_prior * consistency_loss
            loss.backward()
            optimizer_denoise.step()
            optimizer_prior.step()
        print(f"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}")

#############################################
# Inference Pipeline (Sampling using Denoising Network Only)
#############################################
def sample_denoising_net(denoise_model, shape, timesteps=1000,
                         beta_start=0.0001, beta_end=0.02, device="cuda"):
    """
    Generates a sample via the reverse diffusion process.
    
    shape: Output shape (n, 1, H, W).
    """
    denoise_model.eval()
    with torch.no_grad():
        betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
        alphas = 1.0 - betas
        alpha_bars = torch.cumprod(alphas, dim=0)
        x = torch.randn(shape, device=device)
        for t in reversed(range(timesteps)):
            batch_size = shape[0]
            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            v_pred = denoise_model(x, t_tensor)
            beta_t = betas[t]
            mu_t = (1.0 / torch.sqrt(1.0 - beta_t)) * (x - (beta_t / torch.sqrt(1.0 - alpha_bar_t)) * v_pred)
            if t > 0:
                noise = torch.randn_like(x)
                sigma_t = torch.sqrt(beta_t)
                x = mu_t + sigma_t * noise
            else:
                x = mu_t
        return x

#############################################
# Example Usage
#############################################
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Instantiate the two models.
    denoise_model = DenoisingNet(in_channels=1, base_channels=64, time_emb_dim=128, reduction=16)
    prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
    denoise_model.to(device)
    prior_model.to(device)
    
    # Dummy dataloader: yields a list of one batch (for illustration).
    dummy_x0 = torch.randn(8, 1, 32, 32)        # Low-res clean images.
    dummy_prior = torch.randn(8, 3, 128, 128)     # High-res prior images.
    dataloader = [(dummy_x0, dummy_prior)]
    
    # Train for a few epochs.
    train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=5,
                     lambda_prior=0.1, device=device)
    
    # Inference: generate a sample using the denoising network.
    sample_shape = (8, 1, 32, 32)
    generated = sample_denoising_net(denoise_model, sample_shape, timesteps=1000,
                                     beta_start=0.0001, beta_end=0.02, device=device)
    print("Generated sample shape:", generated.shape)






⸻

Explanation
	1.	DenoisingNet:
This network implements a rectified flow–style denoising model. It takes a noisy image x_t and a time step t and predicts the velocity v. We use a series of residual blocks with channel attention (to help highlight faint details).
	2.	PriorConsistencyNet:
A small convolutional network that produces an embedding from an image. During training, we downsample the high–resolution prior (and convert it to grayscale, if necessary) so its spatial dimensions match the denoised image x_0. The L₂ loss between the embeddings of the predicted x_0 and the processed prior enforces consistency.
	3.	Training Pipeline:
For each batch, we sample a time step and generate a noisy image via the forward diffusion process. The denoising network predicts v, which we invert to obtain x_0^{\text{pred}}. The velocity loss is computed between v_\text{pred} and the ground–truth v. Then the prior consistency module computes embeddings of both x_0^{\text{pred}} and a downsampled version of the high–resolution prior, and we compute an L₂ loss between them. The overall loss is the sum of these two (weighted by a factor \lambda), and both models are updated jointly.
	4.	Inference:
During sampling, only the denoising network is used to progressively reverse the diffusion process and generate a final image.

This architecture lets you separately model the denoising and the prior consistency—and co-optimize them—to enforce that the generated image remains faithful to the high–resolution prior while performing well on the rectified flow task.


=====
I need these modifications 

Modify it based on the requests I provide earlier. Now I will provide a list of modifications, read it and understand, ask question before start modifying. Request for modifications are
1. We want to denoise for single step, ie, the noisy image will be a perfect noise free image with one step of Gaussian noise added. Modify the forward pass and training, inferencing. 
2. Although it is one step noise, the noise is heterogeneous. To be specific, the mean and variance of Gaussian is different between columns. （one mean and standard deviation for one column, all rows in that column has same mean and standard deviation). We can train a separate model to ‘predict’ the mean and variance. Or use a predetermined mean/variance map as input.
3. Option to train the model with gpu if available
4. Input image size of 96 by 96 image. Or 32 by 32. It is an hyperparameter.
5. For the model(code blocks) that requires prior. An hyperparameter to determine how much to downsample the prior to match the image resolution. It range from 1 to 8. 
6. After model is train. Add a finetuning model step to update the model so that we can pass the noisy image and iteratively perform denoise step. Doing one step each time. 

====
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

#############################################
# Helper: Time Embedding
#############################################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generates sinusoidal embeddings for time steps.
    
    timesteps: Tensor of shape (n,) containing time step integers.
    embedding_dim: Dimension of the embedding vector.
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

#############################################
# Denoising Network (Rectified Flow, v-prediction)
#############################################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention block.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class ResidualBlockWithAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        Residual block that injects time embedding and applies channel attention.
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb):
        h = self.activation(self.conv1(x))
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        h = self.channel_attention(h)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

class DenoisingNet(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128, reduction=16):
        """
        Rectified flow network for velocity (v) prediction.
        It receives a noisy image and time step, then outputs the predicted velocity.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.downsample = nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1)
        self.resblock2 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.resblock3 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.upsample = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1)
        self.resblock4 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)
    
    def forward(self, x, t):
        """
        x: Noisy image of shape (n, in_channels, H, W)
        t: Time step tensor of shape (n,)
        """
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        h = self.conv_in(x)
        h = self.resblock1(h, t_emb)
        h = self.downsample(h)
        h = self.resblock2(h, t_emb)
        h = self.resblock3(h, t_emb)
        h = self.upsample(h)
        h = self.resblock4(h, t_emb)
        out = self.conv_out(h)
        return out  # Predicted velocity v

#############################################
# Prior Consistency Module
#############################################
class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        """
        A small network that extracts an embedding from an image.
        We use it to compare the denoised image and the prior (after downsampling).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    
    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = self.pool(h)
        h = h.view(h.size(0), -1)
        return h

#############################################
# Training Pipeline: Co-optimizing Two Models
#############################################
def train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=100,
                     lambda_prior=0.1, device="cuda"):
    """
    Co-optimizes:
      - The denoising network (using rectified flow loss).
      - The prior consistency module (enforcing that the denoised image is similar in embedding space to the prior).
    
    dataloader: yields (x0, high_res_prior) where
      • x0: Clean target image (n, 1, H, W).
      • high_res_prior: High-res conditioning image (n, 3, H_hr, W_hr).
    """
    optimizer_denoise = torch.optim.Adam(denoise_model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    
    # Define the diffusion schedule.
    betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
    alphas = 1.0 - betas
    alpha_bars = torch.cumprod(alphas, dim=0)
    
    denoise_model.train()
    prior_model.train()
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            # x0: clean low-res image (n, 1, H, W)
            # high_res_prior: high-res image (n, 3, H_hr, W_hr)
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size = x0.size(0)
            optimizer_denoise.zero_grad()
            optimizer_prior.zero_grad()
            
            # Sample a random time step t.
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            
            # Sample Gaussian noise.
            noise = torch.randn_like(x0)
            # Forward process: x_t = sqrt(alpha_bar_t)*x0 + sqrt(1-alpha_bar_t)*noise
            x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
            # Ground-truth velocity.
            v_true = torch.sqrt(alpha_bar_t) * noise - torch.sqrt(1 - alpha_bar_t) * x0
            # Denoising network prediction.
            v_pred = denoise_model(x_t, t)
            v_loss = F.mse_loss(v_pred, v_true)
            # Invert to obtain predicted clean image.
            x0_pred = torch.sqrt(alpha_bar_t) * x_t - torch.sqrt(1 - alpha_bar_t) * v_pred
            
            # Process the high-res prior: downsample to x0 resolution and convert to grayscale.
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            if prior_down.size(1) == 3 and x0.size(1) == 1:
                prior_down = prior_down.mean(dim=1, keepdim=True)
            
            # Compute embeddings.
            emb_pred = prior_model(x0_pred)
            emb_prior = prior_model(prior_down)
            consistency_loss = F.mse_loss(emb_pred, emb_prior)
            
            loss = v_loss + lambda_prior * consistency_loss
            loss.backward()
            optimizer_denoise.step()
            optimizer_prior.step()
        print(f"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}")

#############################################
# Inference Pipeline (Sampling using Denoising Network Only)
#############################################
def sample_denoising_net(denoise_model, shape, timesteps=1000,
                         beta_start=0.0001, beta_end=0.02, device="cuda"):
    """
    Generates a sample via the reverse diffusion process.
    
    shape: Output shape (n, 1, H, W).
    """
    denoise_model.eval()
    with torch.no_grad():
        betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
        alphas = 1.0 - betas
        alpha_bars = torch.cumprod(alphas, dim=0)
        x = torch.randn(shape, device=device)
        for t in reversed(range(timesteps)):
            batch_size = shape[0]
            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            v_pred = denoise_model(x, t_tensor)
            beta_t = betas[t]
            mu_t = (1.0 / torch.sqrt(1.0 - beta_t)) * (x - (beta_t / torch.sqrt(1.0 - alpha_bar_t)) * v_pred)
            if t > 0:
                noise = torch.randn_like(x)
                sigma_t = torch.sqrt(beta_t)
                x = mu_t + sigma_t * noise
            else:
                x = mu_t
        return x

#############################################
# Example Usage
#############################################
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Instantiate the two models.
    denoise_model = DenoisingNet(in_channels=1, base_channels=64, time_emb_dim=128, reduction=16)
    prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
    denoise_model.to(device)
    prior_model.to(device)
    
    # Dummy dataloader: yields a list of one batch (for illustration).
    dummy_x0 = torch.randn(8, 1, 32, 32)        # Low-res clean images.
    dummy_prior = torch.randn(8, 3, 128, 128)     # High-res prior images.
    dataloader = [(dummy_x0, dummy_prior)]
    
    # Train for a few epochs.
    train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=5,
                     lambda_prior=0.1, device=device)
    
    # Inference: generate a sample using the denoising network.
    sample_shape = (8, 1, 32, 32)
    generated = sample_denoising_net(denoise_model, sample_shape, timesteps=1000,
                                     beta_start=0.0001, beta_end=0.02, device=device)
    print("Generated sample shape:", generated.shape)






⸻

Explanation
	1.	DenoisingNet:
This network implements a rectified flow–style denoising model. It takes a noisy image x_t and a time step t and predicts the velocity v. We use a series of residual blocks with channel attention (to help highlight faint details).
	2.	PriorConsistencyNet:
A small convolutional network that produces an embedding from an image. During training, we downsample the high–resolution prior (and convert it to grayscale, if necessary) so its spatial dimensions match the denoised image x_0. The L₂ loss between the embeddings of the predicted x_0 and the processed prior enforces consistency.
	3.	Training Pipeline:
For each batch, we sample a time step and generate a noisy image via the forward diffusion process. The denoising network predicts v, which we invert to obtain x_0^{\text{pred}}. The velocity loss is computed between v_\text{pred} and the ground–truth v. Then the prior consistency module computes embeddings of both x_0^{\text{pred}} and a downsampled version of the high–resolution prior, and we compute an L₂ loss between them. The overall loss is the sum of these two (weighted by a factor \lambda), and both models are updated jointly.
	4.	Inference:
During sampling, only the denoising network is used to progressively reverse the diffusion process and generate a final image.

This architecture lets you separately model the denoising and the prior consistency—and co-optimize them—to enforce that the generated image remains faithful to the high–resolution prior while performing well on the rectified flow task.


=====
I need these modifications 

Modify it based on the requests I provide earlier. Now I will provide a list of modifications, read it and understand, ask question before start modifying. Request for modifications are
1. We want to denoise for single step, ie, the noisy image will be a perfect noise free image with one step of Gaussian noise added. Modify the forward pass and training, inferencing. 
2. Although it is one step noise, the noise is heterogeneous. To be specific, the mean and variance of Gaussian is different between columns. （one mean and standard deviation for one column, all rows in that column has same mean and standard deviation). We can train a separate model to ‘predict’ the mean and variance. Or use a predetermined mean/variance map as input.
3. Option to train the model with gpu if available
4. Input image size of 96 by 96 image. Or 32 by 32. It is an hyperparameter.
5. For the model(code blocks) that requires prior. An hyperparameter to determine how much to downsample the prior to match the image resolution. It range from 1 to 8. 
6. After model is train. Add a finetuning model step to update the model so that we can pass the noisy image and iteratively perform denoise step. Doing one step each time. 


