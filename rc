import torch
import torch.nn as nn
import torch.nn.functional as F
import math

###############################
# Helper: Time Embedding
###############################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generate sinusoidal embeddings for time steps.
    timesteps: Tensor of shape (n,) containing integer time steps.
    embedding_dim: Dimension of the embedding vector.
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

###############################
# Attention Blocks
###############################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention via global pooling.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class CrossChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Cross-channel attention uses conditioning features to reweight the main features.
        """
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x, cond):
        # cond: conditioning feature map from the prior branch (assumed same channel dim as x)
        b, c, _, _ = cond.size()
        cond_pool = F.adaptive_avg_pool2d(cond, 1).view(b, c)
        attn = self.fc(cond_pool).view(b, c, 1, 1)
        return x * attn

###############################
# Residual Block with Cross–Channel Attention
###############################
class ResidualBlockWithCrossAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        Residual block that:
          1. Processes features with two 3×3 convolutions.
          2. Injects a time embedding.
          3. Applies standard channel attention.
          4. Applies cross–channel attention with conditioning features.
          5. Uses a skip connection (with 1×1 conv if needed).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
        self.cross_attention = CrossChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb, cond=None):
        h = self.activation(self.conv1(x))
        # Inject time embedding.
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        # Apply standard channel attention.
        h = self.channel_attention(h)
        # If conditioning is provided, apply cross-channel attention.
        if cond is not None:
            h = self.cross_attention(h, cond)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

###############################
# Prior Encoder
###############################
class HighResPriorEncoder(nn.Module):
    def __init__(self, in_channels=3, feature_channels=16):
        """
        A simple encoder that downsamples a high–resolution prior image to produce conditioning features.
        """
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, feature_channels, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_channels, feature_channels, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        return self.encoder(x)

###############################
# Main Model: Rectified Flow with Cross–Channel Attention and Prior Conditioning
###############################
class V_Prediction_CrossAttn_Model(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128,
                 prior_in_channels=3, prior_feature_channels=16, reduction=16):
        """
        U–Net for velocity (v) prediction (rectified flow) that incorporates:
          • Time embedding.
          • Residual blocks with both channel and cross–channel attention.
          • Conditioning via a high–resolution prior processed by a prior encoder.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        
        # Prior encoder.
        self.prior_encoder = HighResPriorEncoder(prior_in_channels, prior_feature_channels)
        # Upsample prior features to match spatial resolution of x (assumed to be low-res).
        self.prior_upsample = nn.Sequential(
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # U–Net encoder.
        # We concatenate the input x with upsampled prior features along the channel dimension.
        self.conv_in = nn.Conv2d(in_channels + prior_feature_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithCrossAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.downsample = nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1)
        self.resblock2 = ResidualBlockWithCrossAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.resblock3 = ResidualBlockWithCrossAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        
        # U–Net decoder.
        self.upsample = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1)
        self.resblock4 = ResidualBlockWithCrossAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)
    
    def forward(self, x, t, high_res_prior):
        """
        x: Noisy low–resolution image, shape (n, in_channels, H, W)
        t: Time step tensor, shape (n,)
        high_res_prior: High–resolution prior image, shape (n, prior_in_channels, H_hr, W_hr)
        """
        # Compute and project time embedding.
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        
        # Process high-res prior.
        prior_features = self.prior_encoder(high_res_prior)
        prior_features = self.prior_upsample(prior_features)  # now assumed to be (n, prior_feature_channels, H, W)
        
        # Concatenate the noisy image and prior features.
        x_in = torch.cat([x, prior_features], dim=1)
        
        # Encoder.
        h = self.conv_in(x_in)
        # Here we pass prior features (as cond) to the residual block for cross attention.
        h = self.resblock1(h, t_emb, cond=prior_features)
        h = self.downsample(h)
        h = self.resblock2(h, t_emb, cond=prior_features)
        h = self.resblock3(h, t_emb, cond=prior_features)
        
        # Decoder.
        h = self.upsample(h)
        h = self.resblock4(h, t_emb, cond=prior_features)
        out = self.conv_out(h)
        return out  # This is v_pred (predicted velocity)

###############################
# Training Pipeline (Rectified Flow + Prior Consistency)
###############################
def train_rectified_flow_conditional(model, dataloader, timesteps=1000,
                                     beta_start=0.0001, beta_end=0.02,
                                     epochs=100, lambda_prior=0.1, device="cuda"):
    """
    Trains the rectified flow model with a dual loss:
      • Velocity prediction loss (MSE between predicted and true velocity).
      • Prior consistency loss (L2 distance between estimated x0 and downsampled prior).
      
    dataloader: yields (x0, high_res_prior) where
        x0: clean target image (low–resolution), shape (n, in_channels, H, W)
        high_res_prior: high–resolution prior image, shape (n, prior_in_channels, H_hr, W_hr)
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    # Diffusion schedule.
    betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
    alphas = 1.0 - betas
    alpha_bars = torch.cumprod(alphas, dim=0)
    
    model.train()
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size = x0.size(0)
            optimizer.zero_grad()
            # Sample random timestep t.
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            # Sample Gaussian noise.
            noise = torch.randn_like(x0)
            # Forward diffusion: x_t = sqrt(alpha_bar_t)*x0 + sqrt(1 - alpha_bar_t)*noise
            x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
            # Compute ground truth velocity.
            v_true = torch.sqrt(alpha_bar_t) * noise - torch.sqrt(1 - alpha_bar_t) * x0
            # Predict velocity using our model.
            v_pred = model(x_t, t, high_res_prior)
            v_loss = F.mse_loss(v_pred, v_true)
            # Invert prediction to get an estimate of x0.
            x0_pred = torch.sqrt(alpha_bar_t) * x_t - torch.sqrt(1 - alpha_bar_t) * v_pred
            # Downsample high-res prior to the same resolution as x0 (if needed).
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            prior_loss = F.mse_loss(x0_pred, prior_down)
            loss = v_loss + lambda_prior * prior_loss
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}")

###############################
# Inference Pipeline (Sampling)
###############################
def sample_rectified_flow(model, shape, high_res_prior, timesteps=1000,
                          beta_start=0.0001, beta_end=0.02, device="cuda"):
    """
    Generates a sample using the reverse diffusion (rectified flow) process.
    
    shape: Output shape, e.g. (n, in_channels, H, W)
    high_res_prior: High-resolution prior image.
    """
    model.eval()
    with torch.no_grad():
        high_res_prior = high_res_prior.to(device)
        betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
        alphas = 1.0 - betas
        alpha_bars = torch.cumprod(alphas, dim=0)
        # Start from pure Gaussian noise.
        x_t = torch.randn(shape, device=device)
        for t in reversed(range(1, timesteps)):
            batch_size = shape[0]
            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            # Predict velocity.
            v_pred = model(x_t, t_tensor, high_res_prior)
            # Invert to estimate x0.
            x0_pred = torch.sqrt(alpha_bar_t) * x_t - torch.sqrt(1 - alpha_bar_t) * v_pred
            alpha_bar_prev = alpha_bars[t-1].view(batch_size, 1, 1, 1)
            # Deterministic DDIM update step.
            x_t = (torch.sqrt(alpha_bar_prev) * x0_pred +
                   torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar_t)) *
                   (x_t - torch.sqrt(alpha_bar_t) * x0_pred))
        # At t=0, compute final x0.
        t_tensor = torch.zeros(shape[0], device=device, dtype=torch.long)
        alpha_bar_0 = alpha_bars[0].view(shape[0], 1, 1, 1)
        v_pred = model(x_t, t_tensor, high_res_prior)
        x0_pred = torch.sqrt(alpha_bar_0) * x_t - torch.sqrt(1 - alpha_bar_0) * v_pred
    return x0_pred

###############################
# Example Usage
###############################
if __name__ == "__main__":
    # Assume we work on GPU if available.
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Instantiate the model.
    model = V_Prediction_CrossAttn_Model(in_channels=1, base_channels=64,
                                         time_emb_dim=128, prior_in_channels=3,
                                         prior_feature_channels=16, reduction=16)
    model.to(device)
    
    # Dummy dataloader: yields (x0, high_res_prior).
    # x0: low-res clean image (e.g., 32×32, 1 channel)
    # high_res_prior: high-res conditioning image (e.g., 128×128, 3 channels)
    dummy_x0 = torch.randn(8, 1, 32, 32)
    dummy_prior = torch.randn(8, 3, 128, 128)
    dataloader = [(dummy_x0, dummy_prior)]  # For illustration
    
    # Train for a few epochs.
    train_rectified_flow_conditional(model, dataloader, timesteps=1000,
                                     beta_start=0.0001, beta_end=0.02,
                                     epochs=5, lambda_prior=0.1, device=device)
    
    # Inference: generate a sample conditioned on a dummy prior.
    sample_shape = (8, 1, 32, 32)
    generated = sample_rectified_flow(model, sample_shape, dummy_prior, timesteps=1000,
                                      beta_start=0.0001, beta_end=0.02, device=device)
    print("Generated sample shape:", generated.shape)




⸻

Explanation
	1.	Model Architecture:
– The U–Net uses residual blocks augmented with both a standard channel attention (to reweight intra–feature responses) and a cross–channel attention block (to use conditioning features from the high–resolution prior).
– A separate prior encoder processes the high–resolution prior and then upsamples its features so they can be concatenated with the noisy image.
– A sinusoidal time embedding is injected into each block.
	2.	Training Pipeline:
– For each batch, a random diffusion timestep is sampled, and the noisy image x_t is computed.
– The network predicts the velocity v, and the rectified flow loss is computed as the MSE between v_{\text{pred}} and v_{\text{true}}.
– An additional L₂ loss (prior consistency loss) is computed between the estimated x_0 (inverted from the predicted velocity) and a downsampled version of the high–resolution prior.
– Both losses are combined (with a balancing weight) for backpropagation.
	3.	Inference Pipeline:
– Starting from pure noise, the reverse (sampling) loop iteratively updates the image using the DDIM–style update adapted for rectified flow.
– At each step, the conditioned model predicts v and the estimated x_0 is used to update the latent.

This complete pipeline demonstrates how to co–optimize a rectified flow denoising model with cross–channel attention and a prior consistency loss—all the “tricks” we discussed—to guide the reverse diffusion process with high–resolution prior information.


===
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

#############################################
# Helper: Time Embedding
#############################################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generate sinusoidal embeddings for time steps.
    (Even though we fix t=1 for one-step denoising, this remains for compatibility.)
    
    timesteps: Tensor of shape (n,) with integer time steps.
    embedding_dim: Dimension of the embedding.
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

#############################################
# Attention Blocks
#############################################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention: uses global average pooling and an FC network.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class CrossChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Cross-channel attention: uses conditioning features (from the prior) to reweight the main features.
        """
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x, cond):
        # cond is assumed to have the same channel dimension as x.
        b, c, _, _ = cond.size()
        cond_pool = F.adaptive_avg_pool2d(cond, 1).view(b, c)
        attn = self.fc(cond_pool).view(b, c, 1, 1)
        return x * attn

#############################################
# Residual Block with Cross–Channel Attention
#############################################
class ResidualBlockWithCrossAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        A residual block that:
          - Applies two 3×3 convolutions.
          - Injects a time embedding.
          - Applies standard channel attention.
          - Applies cross-channel attention if conditioning is provided.
          - Uses a skip connection (with 1×1 conv if needed).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
        self.cross_attention = CrossChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb, cond=None):
        h = self.activation(self.conv1(x))
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        h = self.channel_attention(h)
        if cond is not None:
            h = self.cross_attention(h, cond)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

#############################################
# Prior Encoder with Downsampling Hyperparameter
#############################################
class HighResPriorEncoder(nn.Module):
    def __init__(self, in_channels=3, feature_channels=16, downsample_factor=2):
        """
        An encoder that downsamples a high-res prior image.
        
        Args:
            in_channels: Number of channels in the high-res prior.
            feature_channels: Number of channels in the output features.
            downsample_factor: Number of times to downsample by a factor of 2 (range 1 to 8).
        """
        super().__init__()
        layers = []
        for i in range(downsample_factor):
            conv_in = in_channels if i == 0 else feature_channels
            layers.append(nn.Conv2d(conv_in, feature_channels, kernel_size=3, stride=2, padding=1))
            layers.append(nn.ReLU(inplace=True))
        self.encoder = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.encoder(x)

#############################################
# Main Model: V_Prediction_CrossAttn_Model (Conditional One-Step Denoising)
#############################################
class V_Prediction_CrossAttn_Model(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128,
                 prior_in_channels=3, prior_feature_channels=16, downsample_factor=2, reduction=16):
        """
        U–Net for velocity (v) prediction (rectified flow) in a one-step denoising task.
        Incorporates:
          - A sinusoidal time embedding (t is fixed to 1 during training/inference).
          - Residual blocks with channel and cross–channel attention.
          - Conditioning from a high-res prior via a prior encoder.
        
        Args:
            in_channels: Number of channels in the low-res image (1).
            base_channels: Base number of feature channels.
            time_emb_dim: Dimension of the time embedding.
            prior_in_channels: Number of channels in the high-res prior (e.g., 3).
            prior_feature_channels: Number of feature channels output by the prior encoder.
            downsample_factor: How many times to downsample the high-res prior (1 to 8).
            reduction: Reduction factor for attention modules.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        
        # Prior encoder: downsample the high-res prior.
        self.prior_encoder = HighResPriorEncoder(prior_in_channels, prior_feature_channels, downsample_factor)
        # Upsample the extracted prior features to match the low-res image size.
        self.prior_upsample = nn.Sequential(
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # U–Net encoder: concatenate low-res image and upsampled prior features.
        self.conv_in = nn.Conv2d(in_channels + prior_feature_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithCrossAttention(base_channels, base_channels, time_emb_dim, reduction)
        # For one-step denoising, we may keep the network shallow.
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x, t, high_res_prior):
        """
        x: Noisy low-res image, shape (n, 1, H, W)
        t: Time step tensor (should be fixed to 1), shape (n,)
        high_res_prior: High-res prior image, shape (n, 3, H_prior, W_prior)
        """
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        
        # Process high-res prior.
        prior_features = self.prior_encoder(high_res_prior)
        prior_features = self.prior_upsample(prior_features)  # Now shape: (n, prior_feature_channels, H, W)
        
        # Concatenate the noisy image and prior features.
        x_in = torch.cat([x, prior_features], dim=1)
        h = self.conv_in(x_in)
        h = self.resblock1(h, t_emb, cond=prior_features)
        out = self.conv_out(h)
        return out  # Predicted noise (which is used to invert the forward process)

#############################################
# Prior Consistency Module
#############################################
class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        """
        A small network to extract an embedding from an image.
        Used to enforce that the denoised image is consistent with the downsampled prior.
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    
    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = self.pool(h)
        h = h.view(h.size(0), -1)
        return h

#############################################
# Training Pipeline for One-Step Conditional Denoising
#############################################
def train_one_step_conditional(model, prior_model, dataloader, epochs=10, image_size=32,
                               alpha_bar=0.9, mu_range=(-0.1, 0.1), sigma_range=(0.05, 0.2),
                               lambda_prior=0.1, lambda_col=1.0, device="cuda"):
    """
    Train the conditional one-step denoiser.
    
    For each clean image x0 (shape: [batch, 1, image_size, image_size]) and its corresponding high-res prior,
    the forward process is:
         x1 = sqrt(alpha_bar)*x0 + sqrt(1-alpha_bar)*noise,
    where noise is sampled heterogeneously per column:
         For each column, sample a mean and sigma from the given ranges,
         then for every pixel in that column, noise ~ N(mu, sigma^2).
    
    Loss terms:
      - Velocity prediction loss: MSE between predicted noise and actual noise.
      - Column consistency loss: Enforce that predicted noise is constant along rows for each column.
      - Prior consistency loss: MSE between the denoised image (computed from the model’s inversion) and the downsampled prior.
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    model.train()
    prior_model.train()
    sqrt_alpha_bar = math.sqrt(alpha_bar)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            # x0: clean low-res image (batch, 1, image_size, image_size)
            # high_res_prior: high-res prior image (batch, 3, H_prior, W_prior)
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size, _, H, W = x0.shape
            optimizer.zero_grad()
            optimizer_prior.zero_grad()
            
            # Fixed t = 1 for one-step denoising.
            t = torch.ones(batch_size, device=device, dtype=torch.long)
            
            # Generate heterogeneous noise column-wise.
            # For each image, for each column, sample mu and sigma.
            mu = torch.rand(batch_size, 1, 1, W, device=device) * (mu_range[1] - mu_range[0]) + mu_range[0]
            sigma = torch.rand(batch_size, 1, 1, W, device=device) * (sigma_range[1] - sigma_range[0]) + sigma_range[0]
            noise = mu + sigma * torch.randn(batch_size, 1, H, W, device=device)
            
            # Forward process: x1 = sqrt(alpha_bar)*x0 + sqrt(1-alpha_bar)*noise.
            x1 = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise
            
            # Predict noise using the model.
            predicted_noise = model(x1, t, high_res_prior)
            
            # Loss 1: MSE between predicted noise and actual noise.
            loss_mse = F.mse_loss(predicted_noise, noise)
            # Loss 2: Column consistency loss.
            # For each column, compute the mean across rows and enforce predicted noise is close to that mean.
            pred_col_mean = predicted_noise.mean(dim=2, keepdim=True)
            loss_col = F.mse_loss(predicted_noise, pred_col_mean)
            
            # Compute ground-truth velocity: v_true = sqrt(alpha_bar)*noise - sqrt(1-alpha_bar)*x0.
            v_true = sqrt_alpha_bar * noise - sqrt_one_minus_alpha_bar * x0
            # (For our one-step setting, note that predicting noise or velocity are equivalent up to scaling.)
            # Inversion: Estimate the clean image: x0_pred = sqrt(alpha_bar)*x1 - sqrt(1-alpha_bar)*predicted_noise.
            x0_pred = sqrt_alpha_bar * x1 - sqrt_one_minus_alpha_bar * predicted_noise
            
            # Loss 3: Prior consistency loss.
            # Downsample high_res_prior to match x0 size.
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            # If prior is RGB and x0 is one channel, convert by averaging.
            if prior_down.size(1) == 3 and x0.size(1) == 1:
                prior_down = prior_down.mean(dim=1, keepdim=True)
            loss_prior = F.mse_loss(x0_pred, prior_down)
            
            total_loss = loss_mse + lambda_col * loss_col + lambda_prior * loss_prior
            total_loss.backward()
            optimizer.step()
            optimizer_prior.step()
        print(f"Epoch {epoch+1}/{epochs} - Total Loss: {total_loss.item():.6f} | MSE: {loss_mse.item():.6f} | Col: {loss_col.item():.6f} | Prior: {loss_prior.item():.6f}")

#############################################
# Inference Pipeline: One-Step Conditional Denoising
#############################################
def denoise_one_step_conditional(model, x1, high_res_prior, device="cuda"):
    """
    Given a noisy image x1 and a high-res prior, predict the noise (with t=1) and return the denoised image.
    """
    model.eval()
    with torch.no_grad():
        batch_size = x1.size(0)
        t = torch.ones(batch_size, device=device, dtype=torch.long)
        predicted_noise = model(x1, t, high_res_prior)
        # Inversion: x0_pred = sqrt(alpha_bar)*x1 - sqrt(1-alpha_bar)*predicted_noise.
        alpha_bar = 0.9
        sqrt_alpha_bar = math.sqrt(alpha_bar)
        sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
        x0_pred = sqrt_alpha_bar * x1 - sqrt_one_minus_alpha_bar * predicted_noise
    return x0_pred

#############################################
# Iterative Refinement Function
#############################################
def iterative_denoise_conditional(model, x1, high_res_prior, iterations=3, device="cuda"):
    """
    Iteratively apply the one-step conditional denoiser.
    x1: initial noisy image.
    iterations: number of iterative refinement steps (experiment between 1 and 8).
    Returns the final refined image.
    """
    model.eval()
    x_iter = x1.clone().to(device)
    for i in range(iterations):
        x_iter = denoise_one_step_conditional(model, x_iter, high_res_prior, device=device)
    return x_iter

#############################################
# Example Usage
#############################################
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Hyperparameters
    image_size = 32  # You can change to 96 for larger images.
    downsample_factor = 2  # For the prior encoder (range: 1 to 8)
    epochs = 5
    
    # Create a dummy dataloader: a list with one batch of random clean images and corresponding high-res priors.
    dummy_x0 = torch.randn(8, 1, image_size, image_size)           # Clean low-res images.
    dummy_prior = torch.randn(8, 3, image_size * 4, image_size * 4)    # High-res priors (e.g., 4× larger).
    dataloader = [(dummy_x0, dummy_prior)]  # In practice, use a proper DataLoader.
    
    # Instantiate the conditional one-step denoising model and the prior consistency module.
    model = V_Prediction_CrossAttn_Model(in_channels=1, base_channels=64, time_emb_dim=128,
                                         prior_in_channels=3, prior_feature_channels=16,
                                         downsample_factor=downsample_factor, reduction=16)
    prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
    model.to(device)
    prior_model.to(device)
    
    # Train the model.
    train_one_step_conditional(model, prior_model, dataloader, epochs=epochs, image_size=image_size,
                               alpha_bar=0.9, mu_range=(-0.1, 0.1), sigma_range=(0.05, 0.2),
                               lambda_prior=0.1, lambda_col=1.0, device=device)
    
    # Inference: Generate a noisy image using the forward process.
    # For each clean image, generate heterogeneous noise column-wise.
    batch_size, _, H, W = dummy_x0.shape
    mu = torch.rand(batch_size, 1, 1, W, device=device) * (0.1 - (-0.1)) + (-0.1)
    sigma = torch.rand(batch_size, 1, 1, W, device=device) * (0.2 - 0.05) + 0.05
    noise = mu + sigma * torch.randn(batch_size, 1, H, W, device=device)
    sqrt_alpha_bar = math.sqrt(0.9)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - 0.9)
    x1 = sqrt_alpha_bar * dummy_x0.to(device) + sqrt_one_minus_alpha_bar * noise
    
    # One-step denoising.
    x0_pred = denoise_one_step_conditional(model, x1, dummy_prior.to(device), device=device)
    # Iterative refinement: experiment with different iteration counts (e.g., 4 iterations).
    x0_refined = iterative_denoise_conditional(model, x1, dummy_prior.to(device), iterations=4, device=device)
    
    print("One-step denoised image shape:", x0_pred.shape)
    print("Iteratively refined image shape:", x0_refined.shape)




====

import optuna
import torch
import math
import torch.nn.functional as F
import torch.nn as nn

# User-configurable variables
NUM_EPOCHS = 10    # Number of epochs per training run in each trial
NUM_RUNS = 3       # Number of independent runs per trial (for averaging)

# Dummy implementations of the model components and training/inference functions.
# Replace these with your actual implementations.
class V_Prediction_CrossAttn_Model(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128,
                 prior_in_channels=3, prior_feature_channels=16, downsample_factor=2, reduction=16):
        super().__init__()
        # A simplified architecture for demonstration purposes.
        self.conv = nn.Conv2d(in_channels + prior_feature_channels, base_channels, kernel_size=3, padding=1)
        self.out_conv = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)
    
    def forward(self, x, t, high_res_prior):
        # For simplicity, assume high_res_prior is already at the correct size.
        x_in = torch.cat([x, high_res_prior], dim=1)
        h = self.conv(x_in)
        out = self.out_conv(h)
        return out

class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
    
    def forward(self, x):
        return self.conv(x)

def train_one_step_conditional(model, prior_model, dataloader, epochs, image_size,
                               alpha_bar, mu_range, sigma_range,
                               lambda_prior, lambda_col, device="cpu"):
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    model.train()
    prior_model.train()
    sqrt_alpha_bar = math.sqrt(alpha_bar)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size, _, H, W = x0.shape
            optimizer.zero_grad()
            optimizer_prior.zero_grad()
            
            # Fixed time step t = 1.
            t = torch.ones(batch_size, device=device, dtype=torch.long)
            
            # Generate heterogeneous noise column-wise.
            mu = torch.rand(batch_size, 1, 1, W, device=device) * (mu_range[1] - mu_range[0]) + mu_range[0]
            sigma = torch.rand(batch_size, 1, 1, W, device=device) * (sigma_range[1] - sigma_range[0]) + sigma_range[0]
            noise = mu + sigma * torch.randn(batch_size, 1, H, W, device=device)
            
            # Forward process: x1 = sqrt(alpha_bar)*x0 + sqrt(1-alpha_bar)*noise.
            x1 = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise
            
            # Predict noise (or velocity) using the model.
            predicted_noise = model(x1, t, high_res_prior)
            
            # Compute the MSE loss between the predicted and actual noise.
            loss = F.mse_loss(predicted_noise, noise)
            loss.backward()
            optimizer.step()
            optimizer_prior.step()
    # Return loss from the final batch (for simplicity).
    return loss.item()

def denoise_one_step_conditional(model, x1, high_res_prior, alpha_bar, device="cpu"):
    model.eval()
    with torch.no_grad():
        batch_size = x1.size(0)
        t = torch.ones(batch_size, device=device, dtype=torch.long)
        predicted_noise = model(x1, t, high_res_prior)
        sqrt_alpha_bar = math.sqrt(alpha_bar)
        sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
        x0_pred = sqrt_alpha_bar * x1 - sqrt_one_minus_alpha_bar * predicted_noise
    return x0_pred

# Objective function that averages over multiple runs per trial.
def objective(trial):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Suggest hyperparameters.
    alpha_bar = trial.suggest_float("alpha_bar", 0.8, 0.99)
    lambda_prior = trial.suggest_float("lambda_prior", 0.001, 0.5)
    lambda_col = trial.suggest_float("lambda_col", 0.1, 10.0)
    downsample_factor = trial.suggest_int("downsample_factor", 1, 8)
    base_channel = trial.suggest_int("base_channel", 16, 128)
    prior_feature_channels = trial.suggest_int("prior_feature_channels", 8, 64)
    reduction = trial.suggest_int("reduction", 1, min(32, base_channel))
    
    # Fixed parameters for dummy data.
    image_size = 32
    batch_size = 8
    mu_range = (-0.1, 0.1)
    sigma_range = (0.05, 0.2)
    
    # Dummy dataloader: one batch per run.
    dummy_x0 = torch.randn(batch_size, 1, image_size, image_size)
    dummy_prior = torch.randn(batch_size, 3, image_size * 4, image_size * 4)
    dataloader = [(dummy_x0, dummy_prior)]
    
    total_val_loss = 0.0
    
    for run in range(NUM_RUNS):
        # Instantiate model and prior consistency module for each run.
        model = V_Prediction_CrossAttn_Model(
            in_channels=1,
            base_channels=base_channel,
            time_emb_dim=128,
            prior_in_channels=3,
            prior_feature_channels=prior_feature_channels,
            downsample_factor=downsample_factor,
            reduction=reduction
        )
        prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
        model.to(device)
        prior_model.to(device)
        
        # Train model for the specified number of epochs.
        _ = train_one_step_conditional(
            model, prior_model, dataloader,
            epochs=NUM_EPOCHS,
            image_size=image_size,
            alpha_bar=alpha_bar,
            mu_range=mu_range,
            sigma_range=sigma_range,
            lambda_prior=lambda_prior,
            lambda_col=lambda_col,
            device=device
        )
    
        # After training, simulate the forward process to create a noisy input.
        sqrt_alpha_bar = math.sqrt(alpha_bar)
        sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
        H, W = image_size, image_size
        mu_noise = torch.rand(batch_size, 1, 1, W, device=device) * (mu_range[1] - mu_range[0]) + mu_range[0]
        sigma_noise = torch.rand(batch_size, 1, 1, W, device=device) * (sigma_range[1] - sigma_range[0]) + sigma_range[0]
        noise = mu_noise + sigma_noise * torch.randn(batch_size, 1, H, W, device=device)
        x1 = sqrt_alpha_bar * dummy_x0.to(device) + sqrt_one_minus_alpha_bar * noise
        
        # Denoise the image and compute a validation loss.
        x0_pred = denoise_one_step_conditional(model, x1, dummy_prior.to(device), alpha_bar, device=device)
        val_loss = F.mse_loss(x0_pred, dummy_x0.to(device))
        total_val_loss += val_loss.item()
    
    average_val_loss = total_val_loss / NUM_RUNS
    return average_val_loss

if __name__ == "__main__":
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=20)  # Increase n_trials as needed
    
    print("Best trial:")
    best_trial = study.best_trial
    print("  Average Validation Loss: {:.6f}".format(best_trial.value))
    print("  Hyperparameters:")
    for key, value in best_trial.params.items():
        print("    {}: {}".format(key, value))
