import torch
import torch.nn as nn
import torch.nn.functional as F
import math

#############################################
# Helper: Time Embedding
#############################################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generates sinusoidal embeddings for time steps.
    timesteps: Tensor of shape (n,) containing time step integers.
    embedding_dim: Dimension of the embedding vector.
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

#############################################
# Denoising Network (Rectified Flow, v-prediction)
#############################################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention block.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class ResidualBlockWithAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        Residual block that injects time embedding and applies channel attention.
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb):
        h = self.activation(self.conv1(x))
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        h = self.channel_attention(h)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

class DenoisingNet(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128, reduction=16):
        """
        Rectified flow network for velocity (v) prediction.
        It receives a noisy image and time step, then outputs the predicted velocity.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.downsample = nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1)
        self.resblock2 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.resblock3 = ResidualBlockWithAttention(base_channels * 2, base_channels * 2, time_emb_dim, reduction)
        self.upsample = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=4, stride=2, padding=1)
        self.resblock4 = ResidualBlockWithAttention(base_channels, base_channels, time_emb_dim, reduction)
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)
    
    def forward(self, x, t):
        """
        x: Noisy image (n, in_channels, H, W)
        t: Time step tensor (n,)
        """
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        h = self.conv_in(x)
        h = self.resblock1(h, t_emb)
        h = self.downsample(h)
        h = self.resblock2(h, t_emb)
        h = self.resblock3(h, t_emb)
        h = self.upsample(h)
        h = self.resblock4(h, t_emb)
        out = self.conv_out(h)
        return out  # Predicted velocity v

#############################################
# Prior Consistency Module
#############################################
class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        """
        A small network that extracts an embedding from an image.
        We use it to compare the denoised image and the prior (after downsampling).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    
    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(h))
        h = self.pool(h)
        h = h.view(h.size(0), -1)
        return h

#############################################
# Training Pipeline: Co-optimizing Two Models with DPS loss
#############################################
def train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=100,
                     lambda_prior=0.1, device="cuda"):
    """
    Co-optimizes:
      - The denoising network (using rectified flow loss).
      - The prior consistency module (enforcing that the denoised image is similar in embedding space to the prior).
    
    The training loss is augmented with a DPS-inspired term.
    
    dataloader: yields (x0, high_res_prior) where
      • x0: Clean target image (n, 1, H, W).
      • high_res_prior: High-res conditioning image (n, 3, H_hr, W_hr).
    """
    optimizer_denoise = torch.optim.Adam(denoise_model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    
    # Define the diffusion schedule.
    betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
    alphas = 1.0 - betas
    alpha_bars = torch.cumprod(alphas, dim=0)
    
    denoise_model.train()
    prior_model.train()
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            # x0: clean low-res image (n, 1, H, W)
            # high_res_prior: high-res image (n, 3, H_hr, W_hr)
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size = x0.size(0)
            optimizer_denoise.zero_grad()
            optimizer_prior.zero_grad()
            
            # Sample a random time step t.
            t = torch.randint(0, timesteps, (batch_size,), device=device).long()
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            
            # Sample Gaussian noise.
            noise = torch.randn_like(x0)
            # Forward process: x_t = sqrt(alpha_bar_t)*x0 + sqrt(1-alpha_bar_t)*noise
            x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
            # Ground-truth velocity.
            v_true = torch.sqrt(alpha_bar_t) * noise - torch.sqrt(1 - alpha_bar_t) * x0
            # Denoising network prediction.
            v_pred = denoise_model(x_t, t)
            v_loss = F.mse_loss(v_pred, v_true)
            # Invert to obtain predicted clean image.
            x0_pred = torch.sqrt(alpha_bar_t) * x_t - torch.sqrt(1 - alpha_bar_t) * v_pred
            
            # DPS-inspired term: also enforce that the denoised image is close to the prior.
            # Downsample high-res prior to the same resolution as x0.
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            # Convert to grayscale if needed.
            if prior_down.size(1) == 3 and x0.size(1) == 1:
                prior_down = prior_down.mean(dim=1, keepdim=True)
            
            emb_pred = prior_model(x0_pred)
            emb_prior = prior_model(prior_down)
            consistency_loss = F.mse_loss(emb_pred, emb_prior)
            
            loss = v_loss + lambda_prior * consistency_loss
            loss.backward()
            optimizer_denoise.step()
            optimizer_prior.step()
        print(f"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}")

#############################################
# Likelihood Gradient for DPS
#############################################
def likelihood_grad(x, high_res_prior):
    """
    Computes a simple likelihood gradient based on an L2 likelihood.
    We assume a Gaussian likelihood p(y|x) ~ N(prior_down, I), so
      ∇_x log p(y|x) ∝ (prior_down - x)
    where prior_down is the high-res prior downsampled to x's resolution.
    """
    prior_down = F.interpolate(high_res_prior, size=x.shape[-2:], mode='bilinear', align_corners=False)
    if prior_down.size(1) == 3 and x.size(1) == 1:
        prior_down = prior_down.mean(dim=1, keepdim=True)
    return prior_down - x

#############################################
# Inference Pipeline with DPS
#############################################
def sample_dps_two_models(denoise_model, shape, high_res_prior, timesteps=1000,
                          beta_start=0.0001, beta_end=0.02, gamma=0.1, device="cuda"):
    """
    Generates a sample via the reverse diffusion process using DPS.
    
    This function uses the denoising network to predict the velocity,
    and then augments the update with an extra gradient term (γ * grad_data)
    computed from the high-res prior.
    
    shape: Output shape (n, 1, H, W)
    high_res_prior: High-res conditioning image (n, 3, H_hr, W_hr)
    gamma: DPS step size.
    """
    denoise_model.eval()
    with torch.no_grad():
        betas = torch.linspace(beta_start, beta_end, timesteps, device=device)
        alphas = 1.0 - betas
        alpha_bars = torch.cumprod(alphas, dim=0)
        x = torch.randn(shape, device=device)
        for t in reversed(range(timesteps)):
            batch_size = shape[0]
            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)
            alpha_bar_t = alpha_bars[t].view(batch_size, 1, 1, 1)
            # Denoising network prediction.
            v_pred = denoise_model(x, t_tensor)
            beta_t = betas[t]
            mu = (1.0 / torch.sqrt(1.0 - beta_t)) * (x - (beta_t / torch.sqrt(1.0 - alpha_bar_t)) * v_pred)
            # Compute likelihood gradient from high-res prior.
            grad_data = likelihood_grad(x, high_res_prior)
            if t > 0:
                noise = torch.randn_like(x)
                sigma_t = torch.sqrt(beta_t)
                x = mu + sigma_t * noise + gamma * grad_data
            else:
                x = mu + gamma * grad_data
        return x

#############################################
# Example Usage
#############################################
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Instantiate the two models.
    denoise_model = DenoisingNet(in_channels=1, base_channels=64, time_emb_dim=128, reduction=16)
    prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
    denoise_model.to(device)
    prior_model.to(device)
    
    # Dummy dataloader: yields one batch (for illustration).
    dummy_x0 = torch.randn(8, 1, 32, 32)         # Low-res clean images.
    dummy_prior = torch.randn(8, 3, 128, 128)      # High-res prior images.
    dataloader = [(dummy_x0, dummy_prior)]
    
    # Train for a few epochs.
    train_two_models(denoise_model, prior_model, dataloader, timesteps=1000,
                     beta_start=0.0001, beta_end=0.02, epochs=5,
                     lambda_prior=0.1, device=device)
    
    # Inference: generate a sample using DPS.
    sample_shape = (8, 1, 32, 32)
    generated = sample_dps_two_models(denoise_model, sample_shape, dummy_prior,
                                      timesteps=1000, beta_start=0.0001, beta_end=0.02,
                                      gamma=0.1, device=device)
    print("Generated sample shape:", generated.shape)




⸻

Explanation
	1.	Training Phase:
	•	The denoising network is trained in a rectified flow framework to predict velocity v.
	•	The prior consistency module produces embeddings for both the denoised image (obtained by inverting v) and a downsampled version of the high–resolution prior.
	•	The overall loss is the sum of the velocity prediction loss (MSE) and a consistency loss (L₂ loss between embeddings), weighted by a factor \lambda.
	2.	Inference with DPS:
	•	In the reverse diffusion loop, we compute the standard DDPM update using the denoising network’s prediction.
	•	We then compute a likelihood gradient (here defined as \text{prior_down} - x) that nudges the sample toward the high–resolution prior.
	•	The DPS update is performed by adding \gamma times this gradient to the update.

This framework co-optimizes two networks (denoising and prior consistency) during training while also leveraging a DPS‐inspired sampling update during inference to enforce data consistency with the high–resolution prior


+=======.
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

#############################################
# Helper: Time Embedding
#############################################
def get_timestep_embedding(timesteps, embedding_dim):
    """
    Generates sinusoidal embeddings for time steps.
    (Even though for one-step denoising we always use t = 1, this function is kept for compatibility.)
    
    Args:
        timesteps: Tensor of shape (n,) containing time step indices.
        embedding_dim: Dimension of the embedding vector.
        
    Returns:
        Tensor of shape (n, embedding_dim).
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

#############################################
# Attention Blocks
#############################################
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Standard channel attention using global average pooling and a small FC network.
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

class CrossChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction=16):
        """
        Cross-channel attention: uses conditioning features (from the prior) to reweight the main features.
        """
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x, cond):
        # cond is assumed to have the same channel dimension as x.
        b, c, _, _ = cond.size()
        cond_pool = F.adaptive_avg_pool2d(cond, 1).view(b, c)
        attn = self.fc(cond_pool).view(b, c, 1, 1)
        return x * attn

#############################################
# Residual Block with Cross–Channel Attention
#############################################
class ResidualBlockWithCrossAttention(nn.Module):
    def __init__(self, in_channels, out_channels, time_emb_dim, reduction=16):
        """
        A residual block that:
          - Applies two 3×3 convolutions.
          - Injects a time embedding.
          - Applies standard channel attention.
          - Applies cross-channel attention if conditioning is provided.
          - Uses a skip connection (with a 1×1 conv if needed).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.time_emb_proj = nn.Linear(time_emb_dim, out_channels)
        self.activation = nn.ReLU(inplace=True)
        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None
        self.channel_attention = ChannelAttention(out_channels, reduction=reduction)
        self.cross_attention = CrossChannelAttention(out_channels, reduction=reduction)
    
    def forward(self, x, t_emb, cond=None):
        h = self.activation(self.conv1(x))
        t_emb_proj = self.time_emb_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + t_emb_proj
        h = self.conv2(h)
        h = self.channel_attention(h)
        if cond is not None:
            h = self.cross_attention(h, cond)
        if self.skip_conv is not None:
            x = self.skip_conv(x)
        return self.activation(h + x)

#############################################
# Prior Encoder with Downsampling Hyperparameter
#############################################
class HighResPriorEncoder(nn.Module):
    def __init__(self, in_channels=3, feature_channels=16, downsample_factor=2):
        """
        An encoder that downsamples a high-resolution prior image to produce conditioning features.
        
        Args:
            in_channels: Number of channels in the high-res prior.
            feature_channels: Number of channels in the output features.
            downsample_factor: Number of times to downsample by a factor of 2 (range: 1 to 8).
        """
        super().__init__()
        layers = []
        for i in range(downsample_factor):
            conv_in = in_channels if i == 0 else feature_channels
            layers.append(nn.Conv2d(conv_in, feature_channels, kernel_size=3, stride=2, padding=1))
            layers.append(nn.ReLU(inplace=True))
        self.encoder = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.encoder(x)

#############################################
# Main Model: V_Prediction_CrossAttn_Model (Conditional One-Step Denoising)
#############################################
class V_Prediction_CrossAttn_Model(nn.Module):
    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=128,
                 prior_in_channels=3, prior_feature_channels=16, downsample_factor=2, reduction=16):
        """
        U-Net for velocity (v) prediction (rectified flow) for one-step denoising.
        Incorporates:
          - A sinusoidal time embedding (t is fixed to 1 during training/inference).
          - Residual blocks with channel and cross-channel attention.
          - Conditioning from a high-res prior via a prior encoder.
        
        Args:
            in_channels: Number of channels in the low-res image (1).
            base_channels: Base number of feature channels.
            time_emb_dim: Dimension of the time embedding.
            prior_in_channels: Number of channels in the high-res prior (e.g., 3).
            prior_feature_channels: Number of channels output by the prior encoder.
            downsample_factor: How many times to downsample the high-res prior (range: 1 to 8).
            reduction: Reduction factor for attention modules.
        """
        super().__init__()
        self.time_emb_dim = time_emb_dim
        self.time_proj = nn.Linear(time_emb_dim, time_emb_dim)
        
        # Prior encoder: downsample the high-res prior.
        self.prior_encoder = HighResPriorEncoder(prior_in_channels, prior_feature_channels, downsample_factor)
        # Upsample the extracted prior features to match the low-res image size.
        self.prior_upsample = nn.Sequential(
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(prior_feature_channels, prior_feature_channels, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # U-Net encoder: concatenate the low-res image with upsampled prior features.
        self.conv_in = nn.Conv2d(in_channels + prior_feature_channels, base_channels, kernel_size=3, padding=1)
        self.resblock1 = ResidualBlockWithCrossAttention(base_channels, base_channels, time_emb_dim, reduction)
        # For one-step denoising, a shallow network is sufficient.
        self.conv_out = nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x, t, high_res_prior):
        """
        Args:
            x: Noisy low-res image, shape (n, in_channels, H, W)
            t: Time step tensor (should be fixed to 1), shape (n,)
            high_res_prior: High-res conditioning image, shape (n, prior_in_channels, H_prior, W_prior)
        
        Returns:
            Predicted noise (v) for inversion.
        """
        t_emb = get_timestep_embedding(t, self.time_emb_dim)
        t_emb = self.time_proj(t_emb)
        
        # Process high-res prior.
        prior_features = self.prior_encoder(high_res_prior)
        prior_features = self.prior_upsample(prior_features)  # shape: (n, prior_feature_channels, H, W)
        
        # Concatenate the noisy image and upsampled prior features.
        x_in = torch.cat([x, prior_features], dim=1)
        h = self.conv_in(x_in)
        h = self.resblock1(h, t_emb, cond=prior_features)
        out = self.conv_out(h)
        return out  # Predicted noise

#############################################
# Prior Consistency Module
#############################################
class PriorConsistencyNet(nn.Module):
    def __init__(self, in_channels=1, embed_dim=64):
        """
        A small network that extracts an embedding from an image.
        We use it to compare the denoised image and the prior (after downsampling).
        """
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, embed_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
    
    def forward(self, x):
        h = F.relu(self.conv1(x))
        h = F.relu(self.conv2(x))
        h = self.pool(h)
        h = h.view(h.size(0), -1)
        return h

#############################################
# Training Pipeline: Co-optimizing Two Models with DPS loss
#############################################
def train_two_models(denoise_model, prior_model, dataloader, epochs=100, image_size=32,
                     alpha_bar=0.9, mu_range=(-0.1, 0.1), sigma_range=(0.05, 0.2),
                     lambda_prior=0.1, lambda_col=1.0, device="cuda"):
    """
    Train the conditional one-step denoiser.
    
    For each clean image x₀ (shape: [batch, 1, image_size, image_size]) and its high-res prior,
    the forward process is defined as:
         x₁ = sqrt(alpha_bar)*x₀ + sqrt(1-alpha_bar)*noise,
    where noise is generated heterogeneously per column:
         For each image and each column, sample a mean and sigma from the given ranges,
         then for every pixel in that column, noise ~ N(μ, σ²).
         
    Losses:
      - MSE loss between predicted noise and actual noise.
      - Column consistency loss: the predicted noise should be nearly constant along rows for each column.
      - Prior consistency loss: MSE loss between the denoised image (via inversion) and the downsampled high-res prior.
    """
    optimizer = torch.optim.Adam(denoise_model.parameters(), lr=1e-4)
    optimizer_prior = torch.optim.Adam(prior_model.parameters(), lr=1e-4)
    denoise_model.train()
    prior_model.train()
    sqrt_alpha_bar = math.sqrt(alpha_bar)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
    
    for epoch in range(epochs):
        for x0, high_res_prior in dataloader:
            # x₀: clean low-res image (batch, 1, image_size, image_size)
            # high_res_prior: high-res image (batch, 3, H_prior, W_prior)
            x0 = x0.to(device)
            high_res_prior = high_res_prior.to(device)
            batch_size, _, H, W = x0.shape
            optimizer.zero_grad()
            optimizer_prior.zero_grad()
            
            # Fixed t = 1 for one-step denoising.
            t = torch.ones(batch_size, device=device, dtype=torch.long)
            
            # Generate heterogeneous noise column-wise.
            mu = torch.rand(batch_size, 1, 1, W, device=device) * (mu_range[1] - mu_range[0]) + mu_range[0]
            sigma = torch.rand(batch_size, 1, 1, W, device=device) * (sigma_range[1] - sigma_range[0]) + sigma_range[0]
            noise = mu + sigma * torch.randn(batch_size, 1, H, W, device=device)
            
            # Forward process: x₁ = sqrt(alpha_bar)*x₀ + sqrt(1-alpha_bar)*noise.
            x1 = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise
            
            # Predict noise using the model.
            predicted_noise = denoise_model(x1, t, high_res_prior)
            loss_mse = F.mse_loss(predicted_noise, noise)
            # Column consistency loss: enforce that predicted noise is nearly constant along rows.
            pred_col_mean = predicted_noise.mean(dim=2, keepdim=True)
            loss_col = F.mse_loss(predicted_noise, pred_col_mean)
            
            # Inversion: x₀_pred = sqrt(alpha_bar)*x₁ - sqrt(1-alpha_bar)*predicted_noise.
            x0_pred = sqrt_alpha_bar * x1 - sqrt_one_minus_alpha_bar * predicted_noise
            
            # Prior consistency: downsample high_res_prior to match x₀ resolution.
            prior_down = F.interpolate(high_res_prior, size=x0.shape[-2:], mode='bilinear', align_corners=False)
            if prior_down.size(1) == 3 and x0.size(1) == 1:
                prior_down = prior_down.mean(dim=1, keepdim=True)
            loss_prior = F.mse_loss(prior_model(x0_pred), prior_model(prior_down))
            
            total_loss = loss_mse + lambda_col * loss_col + lambda_prior * loss_prior
            total_loss.backward()
            optimizer.step()
            optimizer_prior.step()
        print(f"Epoch {epoch+1}/{epochs} - Total Loss: {total_loss.item():.6f} | MSE: {loss_mse.item():.6f} | Col: {loss_col.item():.6f} | Prior: {loss_prior.item():.6f}")

#############################################
# Likelihood Gradient for DPS
#############################################
def likelihood_grad(x, high_res_prior):
    """
    Computes a simple likelihood gradient based on an L2 likelihood.
    We assume a Gaussian likelihood p(y|x) ~ N(prior_down, I), so
      ∇_x log p(y|x) ∝ (prior_down - x)
    where prior_down is the high-res prior downsampled to x's resolution.
    """
    prior_down = F.interpolate(high_res_prior, size=x.shape[-2:], mode='bilinear', align_corners=False)
    if prior_down.size(1) == 3 and x.size(1) == 1:
        prior_down = prior_down.mean(dim=1, keepdim=True)
    return prior_down - x

#############################################
# Inference Pipeline: DPS for Rectified Flow with Prior Conditioning
#############################################
def sample_dps(denoise_model, shape, high_res_prior, timesteps=1000,
               beta_start=0.0001, beta_end=0.02, gamma=0.1, device="cuda"):
    """
    Generates a sample via the reverse diffusion process with DPS.
    The update is augmented with a likelihood gradient computed from the high-res prior.
    
    Args:
        denoise_model: The trained denoising network.
        shape: Output shape (n, 1, H, W).
        high_res_prior: High-res conditioning image (n, 3, H_prior, W_prior).
        gamma: DPS step size.
        device: Device for computation.
    
    Returns:
        Generated sample (n, 1, H, W).
    """
    denoise_model.eval()
    with torch.no_grad():
        # For one-step denoising, use fixed alpha_bar.
        alpha_bar = 0.9
        sqrt_alpha_bar = math.sqrt(alpha_bar)
        sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
        # Start from pure noise.
        x = torch.randn(shape, device=device)
        # Single reverse update.
        t_tensor = torch.ones(shape[0], device=device, dtype=torch.long)
        predicted_noise = denoise_model(x, t_tensor, high_res_prior)
        mu = (1.0 / sqrt_alpha_bar) * (x - sqrt_one_minus_alpha_bar * predicted_noise)
        # Likelihood gradient.
        grad_data = likelihood_grad(x, high_res_prior)
        x_final = mu + gamma * grad_data
        return x_final

#############################################
# Iterative Refinement Function (Conditional)
#############################################
def iterative_denoise_conditional(denoise_model, x1, high_res_prior, iterations=3, alpha_bar=0.9, device="cuda"):
    """
    Iteratively apply the one-step conditional denoiser.
    
    Args:
        denoise_model: The trained conditional one-step denoiser.
        x1: Initial noisy image.
        high_res_prior: High-res conditioning image.
        iterations: Number of refinement iterations (experiment between 1 and 8).
        alpha_bar: Diffusion parameter for one-step denoising.
        device: Device for computation.
    
    Returns:
        Final refined image.
    """
    denoise_model.eval()
    x_iter = x1.clone().to(device)
    batch_size = x_iter.size(0)
    t = torch.ones(batch_size, device=device, dtype=torch.long)
    sqrt_alpha_bar = math.sqrt(alpha_bar)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - alpha_bar)
    with torch.no_grad():
        for i in range(iterations):
            predicted_noise = denoise_model(x_iter, t, high_res_prior)
            x_iter = sqrt_alpha_bar * x_iter - sqrt_one_minus_alpha_bar * predicted_noise
    return x_iter

#############################################
# Example Usage
#############################################
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Hyperparameters.
    image_size = 32  # or 96, etc.
    epochs = 5
    # Dummy dataloader: a list with one batch of random clean images and corresponding high-res priors.
    dummy_x0 = torch.randn(8, 1, image_size, image_size)           # Clean low-res images.
    dummy_prior = torch.randn(8, 3, image_size*4, image_size*4)        # High-res priors (e.g., 4× larger).
    dataloader = [(dummy_x0, dummy_prior)]  # In practice, use a proper DataLoader.
    
    # Instantiate the conditional one-step denoising model and the prior consistency module.
    model = V_Prediction_CrossAttn_Model(in_channels=1, base_channels=64, time_emb_dim=128,
                                         prior_in_channels=3, prior_feature_channels=16, reduction=16)
    prior_model = PriorConsistencyNet(in_channels=1, embed_dim=64)
    model.to(device)
    prior_model.to(device)
    
    # Train for a few epochs.
    train_two_models(model, prior_model, dataloader, epochs=epochs, image_size=image_size,
                     alpha_bar=0.9, mu_range=(-0.1, 0.1), sigma_range=(0.05, 0.2),
                     lambda_prior=0.1, lambda_col=1.0, device=device)
    
    # Inference: Generate a noisy image using the forward process.
    batch_size, _, H, W = dummy_x0.shape
    # Generate heterogeneous noise column-wise.
    mu = torch.rand(batch_size, 1, 1, W, device=device) * (0.1 - (-0.1)) + (-0.1)
    sigma = torch.rand(batch_size, 1, 1, W, device=device) * (0.2 - 0.05) + 0.05
    noise = mu + sigma * torch.randn(batch_size, 1, H, W, device=device)
    sqrt_alpha_bar = math.sqrt(0.9)
    sqrt_one_minus_alpha_bar = math.sqrt(1 - 0.9)
    x1 = sqrt_alpha_bar * dummy_x0.to(device) + sqrt_one_minus_alpha_bar * noise
    
    # One-step denoising.
    x0_pred = denoise_one_step_conditional(model, x1, dummy_prior.to(device), alpha_bar=0.9, device=device)
    # Iterative refinement: e.g., 4 iterations.
    x0_refined = iterative_denoise_conditional(model, x1, dummy_prior.to(device), iterations=4, alpha_bar=0.9, device=device)
    
    print("One-step denoised image shape:", x0_pred.shape)
    print("Iteratively refined image shape:", x0_refined.shape)