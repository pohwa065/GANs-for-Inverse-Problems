import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# Sinusoidal timestep embedding
def get_timestep_embedding(timesteps, embedding_dim):
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)
    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:
        emb = F.pad(emb, (0, 1))
    return emb

# 1. Column-wise statistical encoder
class ColumnStatEncoder(nn.Module):
    def __init__(self, in_ch=1, out_ch=32):
        super().__init__()
        self.proj = nn.Conv2d(2, out_ch, kernel_size=1)

    def forward(self, x):
        col_mean = x.mean(dim=2, keepdim=True)  # (B, 1, 1, W)
        col_std = x.std(dim=2, keepdim=True)    # (B, 1, 1, W)
        stats = torch.cat([col_mean, col_std], dim=1)  # (B, 2, 1, W)
        return self.proj(stats)  # (B, out_ch, 1, W)

# 2. Column-wise self-attention block
class ColumnSelfAttention(nn.Module):
    def __init__(self, in_ch, heads=4):
        super().__init__()
        self.norm = nn.LayerNorm(in_ch)
        self.attn = nn.MultiheadAttention(embed_dim=in_ch, num_heads=heads, batch_first=True)

    def forward(self, x):
        B, C, H, W = x.shape
        x_perm = x.permute(0, 2, 3, 1).reshape(B * H, W, C)
        x_norm = self.norm(x_perm)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        out = attn_out.reshape(B, H, W, C).permute(0, 3, 1, 2)
        return out

# 3. Cross-column attention
class CrossColumnAttention(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.attn = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, None)),
            nn.Conv2d(in_ch, in_ch, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        weight = self.attn(x)  # (B, C, 1, W)
        return x * weight

# 4. Residual block with time embedding and cross-attention
class ResidualBlockWithCrossAttention(nn.Module):
    def __init__(self, in_ch, out_ch, time_emb_dim):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)
        self.time_proj = nn.Linear(time_emb_dim, out_ch)
        self.cross_attn = CrossColumnAttention(out_ch)
        self.skip_conv = nn.Conv2d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()
        self.act = nn.ReLU(inplace=True)

    def forward(self, x, t_emb):
        h = self.act(self.conv1(x))
        h = h + self.time_proj(t_emb).unsqueeze(-1).unsqueeze(-1)
        h = self.conv2(h)
        h = self.cross_attn(h)
        return self.act(h + self.skip_conv(x))

# 5. Full model
class VelocityUNet(nn.Module):
    def __init__(self, in_ch=1, base_ch=64, time_dim=128):
        super().__init__()
        self.time_proj = nn.Linear(time_dim, time_dim)
        self.col_embed = ColumnStatEncoder(in_ch=1, out_ch=base_ch)

        self.down1 = nn.Sequential(
            nn.Conv2d(in_ch, base_ch, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.attn1 = ColumnSelfAttention(base_ch)

        self.down2 = nn.Sequential(
            nn.Conv2d(base_ch, base_ch * 2, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
        self.attn2 = ColumnSelfAttention(base_ch * 2)

        self.down3 = nn.Sequential(
            nn.Conv2d(base_ch * 2, base_ch * 4, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )

        self.up1 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(base_ch * 4, base_ch * 2, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.up2 = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
            nn.Conv2d(base_ch * 2, base_ch, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

        self.res = ResidualBlockWithCrossAttention(in_ch=2*base_ch, out_ch=base_ch, time_emb_dim=time_dim)
        self.out = nn.Conv2d(base_ch, in_ch, kernel_size=3, padding=1)

    def forward(self, x, t):
        t_emb = get_timestep_embedding(t, self.time_proj.in_features)
        t_emb = self.time_proj(t_emb)

        col_feat = self.col_embed(x)  # shape: (B, base_ch, 1, W)

        d1 = self.attn1(self.down1(x)) + col_feat
        d2 = self.attn2(self.down2(d1))
        d3 = self.down3(d2)
        u1 = self.up1(d3)
        u2 = self.up2(u1)

        skip = torch.cat([d1, u2], dim=1)
        fused = self.res(skip, t_emb)
        return self.out(fused)